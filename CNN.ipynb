{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import re\n",
    "import csv\n",
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The function \"text_to_wordlist\" is adapted from:\n",
    "# kaggle.com/currie32/quora-question-pairs/the-importance-of-cleaning-text\n",
    "\n",
    "def text_to_wordlist(\n",
    "                     question, \n",
    "                     remove_stopwords   = False, \n",
    "                     stem_words         = False,\n",
    "                     remove_punctuation = False\n",
    "                    ):\n",
    "\n",
    "    question = re.sub( r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \"          , question)\n",
    "    question = re.sub( r\"what's\"                , \"what is \"   , question)\n",
    "    question = re.sub( r\"\\'s\"                   , \" \"          , question)\n",
    "    question = re.sub( r\"\\'ve\"                  , \" have \"     , question)\n",
    "    question = re.sub( r\"can't\"                 , \"cannot \"    , question)\n",
    "    question = re.sub( r\"n't\"                   , \" not \"      , question)\n",
    "    question = re.sub( r\"i'm\"                   , \"i am \"      , question)\n",
    "    question = re.sub( r\"\\'re\"                  , \" are \"      , question)\n",
    "    question = re.sub( r\"\\'d\"                   , \" would \"    , question)\n",
    "    question = re.sub( r\"\\'ll\"                  , \" will \"     , question)\n",
    "    question = re.sub( r\",\"                     , \" \"          , question)\n",
    "    question = re.sub( r\"\\.\"                    , \" \"          , question)\n",
    "    question = re.sub( r\"!\"                     , \" ! \"        , question)\n",
    "    question = re.sub( r\"\\/\"                    , \" \"          , question)\n",
    "    question = re.sub( r\"\\^\"                    , \" ^ \"        , question)\n",
    "    question = re.sub( r\"\\+\"                    , \" + \"        , question)\n",
    "    question = re.sub( r\"\\-\"                    , \" - \"        , question)\n",
    "    question = re.sub( r\"\\=\"                    , \" = \"        , question)\n",
    "    question = re.sub( r\"'\"                     , \" \"          , question)\n",
    "    question = re.sub( r\"(\\d+)(k)\"              , r\"\\g<1>000\"  , question)\n",
    "    question = re.sub( r\":\"                     , \" : \"        , question)\n",
    "    question = re.sub( r\" e g \"                 , \" eg \"       , question)\n",
    "    question = re.sub( r\" b g \"                 , \" bg \"       , question)\n",
    "    question = re.sub( r\" u s \"                 , \" american \" , question)\n",
    "    question = re.sub( r\"\\0s\"                   , \"0\"          , question)\n",
    "    question = re.sub( r\" 9 11 \"                , \"911\"        , question)\n",
    "    question = re.sub( r\"e - mail\"              , \"email\"      , question)\n",
    "    question = re.sub( r\"j k\"                   , \"jk\"         , question)\n",
    "    question = re.sub( r\"\\s{2,}\"                , \" \"          , question)\n",
    "\n",
    "    if remove_stopwords:\n",
    "        question = question.lower().split()\n",
    "        question = [w for w in question if not w in nltk.corpus.stopwords.words(\"english\")]\n",
    "        question = ' '.join(question)\n",
    "\n",
    "    if stem_words:        \n",
    "        question      = question.lower().split()\n",
    "        stemmed_words = [nltk.stem.SnowballStemmer('english').stemmer.stem(word) for word in question]\n",
    "        question      = ' '.join(stemmed_words)\n",
    "\n",
    "    if remove_punctuation:\n",
    "        question = ''.join([character for character in question if character not in string.punctuation])\n",
    "    \n",
    "    return(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_data_pd = pd.read_csv( '/home/ubuntu/train.csv' )\n",
    "testing_data_pd  = pd.read_csv( '/home/ubuntu/test.csv'  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing training dataset\n",
      "Found 404290 question pairs in train.csv\n",
      "Processing test dataset\n",
      "Found 2345796 question pairs in test.csv\n"
     ]
    }
   ],
   "source": [
    "####################################################\n",
    "\n",
    "print 'Processing training dataset'\n",
    "\n",
    "training_questions_1 = [] \n",
    "training_questions_2 = []\n",
    "training_labels      = []\n",
    "\n",
    "with open( '/home/ubuntu/train.csv' ) as f:\n",
    "    \n",
    "    reader = csv.reader(f, delimiter=',')\n",
    "    header = next(reader)\n",
    "    \n",
    "    for row in reader:\n",
    "        \n",
    "        training_questions_1.append( text_to_wordlist ( row[3] ))\n",
    "        training_questions_2.append( text_to_wordlist ( row[4] ))\n",
    "        training_labels.append( int(row[5] ))\n",
    "        \n",
    "print 'Found %s question pairs in train.csv' % len(training_questions_1)\n",
    "\n",
    "####################################################\n",
    "\n",
    "print 'Processing test dataset'\n",
    "\n",
    "test_questions_1  = []\n",
    "test_questions_2  = []\n",
    "test_question_ids = []\n",
    "\n",
    "with open( '/home/ubuntu/test.csv' ) as f:\n",
    "    \n",
    "    reader = csv.reader(f, delimiter=',')\n",
    "    header = next(reader)\n",
    "    \n",
    "    for row in reader:\n",
    "        \n",
    "        test_questions_1.append( text_to_wordlist ( row[1] ))\n",
    "        test_questions_2.append( text_to_wordlist ( row[2] ))\n",
    "        test_question_ids.append( row[0] )\n",
    "        \n",
    "print 'Found %s question pairs in test.csv' % len(test_questions_1)\n",
    "\n",
    "####################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing Words with KERAS\n",
      "Found 120539 unique word tokens\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "####################################################\n",
    "\n",
    "print 'Tokenizing Words with KERAS'\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts ( \n",
    "                        training_questions_1 + \n",
    "                        training_questions_2 + \n",
    "                        test_questions_1  +\n",
    "                        test_questions_2\n",
    "                       )\n",
    "\n",
    "training_sequences_1 = tokenizer.texts_to_sequences( training_questions_1 )\n",
    "training_sequences_2 = tokenizer.texts_to_sequences( training_questions_2 )\n",
    "testing_sequences_1  = tokenizer.texts_to_sequences( test_questions_1 )\n",
    "testing_sequences_2  = tokenizer.texts_to_sequences( test_questions_2 )\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "print 'Found %s unique word tokens' % len(word_index)\n",
    "\n",
    "####################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Tensors with KERAS\n",
      "Shape of training data tensor: (404290, 36)\n",
      "Shape of testing data tensor: (2345796, 36)\n",
      "Number of labels: 404290\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "maximum_sequence_length = 36\n",
    "\n",
    "####################################################\n",
    "\n",
    "print 'Creating Tensors with KERAS'\n",
    "\n",
    "training_data_1   = pad_sequences (\n",
    "                                   sequences  = training_sequences_1, \n",
    "                                   maxlen     = maximum_sequence_length\n",
    "                                  )\n",
    "\n",
    "training_data_2   = pad_sequences (\n",
    "                                   sequences  = training_sequences_2, \n",
    "                                   maxlen     = maximum_sequence_length \n",
    "                                  )\n",
    "\n",
    "testing_data_1    = pad_sequences (\n",
    "                                   sequences  = testing_sequences_1, \n",
    "                                   maxlen     = maximum_sequence_length,\n",
    "                                   padding    = 'post',\n",
    "                                   truncating = 'post'\n",
    "                                  )\n",
    "\n",
    "testing_data_2    = pad_sequences ( \n",
    "                                   sequences  = testing_sequences_2, \n",
    "                                   maxlen     = maximum_sequence_length,\n",
    "                                   padding    = 'post',\n",
    "                                   truncating = 'post'\n",
    "                                  )\n",
    "\n",
    "training_labels   = training_data_pd.is_duplicate\n",
    "\n",
    "print 'Shape of training data tensor:',  training_data_1.shape\n",
    "print 'Shape of testing data tensor:',   testing_data_1.shape\n",
    "print 'Number of labels:',               len(training_labels)\n",
    "\n",
    "####################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors\n",
      "Found 2196017 word vectors of word2vec\n"
     ]
    }
   ],
   "source": [
    "####################################################\n",
    "\n",
    "print 'Indexing word vectors'\n",
    "\n",
    "embeddings_index = {}\n",
    "\n",
    "with open( '/home/ubuntu/glove.840B.300d.txt' ) as f:\n",
    "\n",
    "    for line in f:\n",
    "\n",
    "        word_embeddings        = line.split(' ')\n",
    "        word                   = word_embeddings[0]\n",
    "        embedding              = np.asarray( word_embeddings[1:], dtype='float32' )\n",
    "        embeddings_index[word] = embedding\n",
    "\n",
    "print 'Found %s word vectors of word2vec' % len(embeddings_index)\n",
    "\n",
    "####################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing embedding matrix\n",
      "Embedding Matrix Shape: (120540, 300)\n"
     ]
    }
   ],
   "source": [
    "embedding_dimension = 300\n",
    "\n",
    "####################################################\n",
    "\n",
    "print 'Preparing embedding matrix'\n",
    "\n",
    "number_of_words = len(word_index)\n",
    "\n",
    "embedding_matrix = np.zeros( (number_of_words + 1, embedding_dimension) )\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    \n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    \n",
    "    if embedding_vector is not None:\n",
    "        \n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "print 'Embedding Matrix Shape:', embedding_matrix.shape\n",
    "\n",
    "####################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared model parameters\n"
     ]
    }
   ],
   "source": [
    "####################################################\n",
    "\n",
    "print 'Prepared model parameters'\n",
    "\n",
    "number_nodes       = 128  # Number of nodes in the Dense layers\n",
    "percentage_dropout = 0.25 # Percentage of nodes to drop\n",
    "number_filters     = 32   # Number of filters to use in Convolution1D\n",
    "filter_length      = 3    # Length of filter for Convolution1D\n",
    "\n",
    "weights            = keras.initializers.TruncatedNormal (\n",
    "                                                         mean   = 0.0, \n",
    "                                                         stddev = 0.05, \n",
    "                                                         seed   = 2\n",
    "                                                        )\n",
    "bias               = bias_initializer = 'zeros'\n",
    "\n",
    "####################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defined the model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/IPython/kernel/__main__.py:113: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/usr/local/lib/python2.7/dist-packages/IPython/kernel/__main__.py:140: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/usr/local/lib/python2.7/dist-packages/IPython/kernel/__main__.py:167: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "merge_3 (Merge)              (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1)                 129       \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 1)                 4         \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 145,825,565\n",
      "Trainable params: 1,173,547\n",
      "Non-trainable params: 144,652,018\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "\n",
    "####################################################\n",
    "\n",
    "print 'Defined the model'\n",
    "\n",
    "model1 = Sequential()\n",
    "\n",
    "model1.add(keras.layers.Embedding (\n",
    "                                   input_dim    = number_of_words + 1,\n",
    "                                   output_dim   = embedding_dimension,\n",
    "                                   weights      = [embedding_matrix],\n",
    "                                   input_length = maximum_sequence_length,\n",
    "                                   trainable    = False\n",
    "                                  ))\n",
    "\n",
    "model1.add(keras.layers.Convolution1D (\n",
    "                                       filters     = number_filters, \n",
    "                                       kernel_size = filter_length, \n",
    "                                       padding     = 'same'\n",
    "                                      ))\n",
    "\n",
    "model1.add(keras.layers.BatchNormalization())\n",
    "model1.add(keras.layers.Activation( 'relu' ))\n",
    "model1.add(keras.layers.Dropout( percentage_dropout ))\n",
    "\n",
    "model1.add(keras.layers.Convolution1D (\n",
    "                                       filters     = number_filters, \n",
    "                                       kernel_size = filter_length, \n",
    "                                       padding     = 'same'\n",
    "                                      ))\n",
    "\n",
    "model1.add(keras.layers.BatchNormalization())\n",
    "model1.add(keras.layers.Activation('relu'))\n",
    "model1.add(keras.layers.Dropout( percentage_dropout ))\n",
    "\n",
    "model1.add(keras.layers.Flatten())\n",
    "\n",
    "model2 = Sequential()\n",
    "\n",
    "model2.add(keras.layers.Embedding (\n",
    "                                   input_dim    = number_of_words + 1,\n",
    "                                   output_dim   = embedding_dimension,\n",
    "                                   weights      = [embedding_matrix],\n",
    "                                   input_length = maximum_sequence_length,\n",
    "                                   trainable    = False\n",
    "                                  ))\n",
    "\n",
    "model2.add(keras.layers.Convolution1D (\n",
    "                                       filters     = number_filters, \n",
    "                                       kernel_size = filter_length, \n",
    "                                       padding     = 'same'\n",
    "                                      ))\n",
    "\n",
    "model2.add(keras.layers.BatchNormalization())\n",
    "model2.add(keras.layers.Activation('relu'))\n",
    "model2.add(keras.layers.Dropout( percentage_dropout ))\n",
    "\n",
    "model2.add(keras.layers.Convolution1D (\n",
    "                                       filters     = number_filters, \n",
    "                                       kernel_size = filter_length, \n",
    "                                       padding     = 'same'\n",
    "                                      ))\n",
    "\n",
    "model2.add(keras.layers.BatchNormalization())\n",
    "model2.add(keras.layers.Activation('relu'))\n",
    "model2.add(keras.layers.Dropout( percentage_dropout ))\n",
    "\n",
    "model2.add(keras.layers.Flatten())\n",
    "\n",
    "model3 = Sequential()\n",
    "\n",
    "model3.add(keras.layers.Embedding (\n",
    "                                   input_dim    = number_of_words + 1,\n",
    "                                   output_dim   = embedding_dimension,\n",
    "                                   weights      = [embedding_matrix],\n",
    "                                   input_length = maximum_sequence_length,\n",
    "                                   trainable    = False\n",
    "                                  ))\n",
    "\n",
    "model3.add(keras.layers.TimeDistributed(keras.layers.Dense( embedding_dimension )))\n",
    "model3.add(keras.layers.BatchNormalization())\n",
    "model3.add(keras.layers.Activation('relu'))\n",
    "model3.add(keras.layers.Dropout( percentage_dropout ))\n",
    "model3.add(keras.layers.Lambda (\n",
    "                                lambda x: keras.backend.max(x, axis=1), \n",
    "                                output_shape = ( embedding_dimension, )\n",
    "                               ))\n",
    "\n",
    "model4 = Sequential()\n",
    "\n",
    "model4.add(keras.layers.Embedding (\n",
    "                                   input_dim    = number_of_words + 1,\n",
    "                                   output_dim   = embedding_dimension,\n",
    "                                   weights      = [embedding_matrix],\n",
    "                                   input_length = maximum_sequence_length,\n",
    "                                   trainable    = False\n",
    "                                  ))\n",
    "\n",
    "model4.add(keras.layers.TimeDistributed(keras.layers.Dense( embedding_dimension )))\n",
    "model4.add(keras.layers.BatchNormalization())\n",
    "model4.add(keras.layers.Activation('relu'))\n",
    "model4.add(keras.layers.Dropout( percentage_dropout ))\n",
    "model4.add(keras.layers.Lambda (\n",
    "                                lambda x: keras.backend.max(x, axis=1), \n",
    "                                output_shape = ( embedding_dimension, )\n",
    "                               ))\n",
    "\n",
    "modela = Sequential()\n",
    "\n",
    "modela.add(keras.layers.Merge (\n",
    "                               [model1, model2], \n",
    "                               mode = 'concat'\n",
    "                              ))\n",
    "\n",
    "modela.add(keras.layers.Dense (\n",
    "                               units              = number_nodes * 2, \n",
    "                               kernel_initializer = weights, \n",
    "                               bias_initializer   = bias\n",
    "                              ))\n",
    "\n",
    "modela.add(keras.layers.BatchNormalization())\n",
    "modela.add(keras.layers.Activation('relu'))\n",
    "modela.add(keras.layers.Dropout(percentage_dropout))\n",
    "\n",
    "modela.add(keras.layers.Dense (\n",
    "                               units              = number_nodes, \n",
    "                               kernel_initializer = weights, \n",
    "                               bias_initializer   = bias\n",
    "                              ))\n",
    "\n",
    "modela.add(keras.layers.BatchNormalization())\n",
    "modela.add(keras.layers.Activation('relu'))\n",
    "modela.add(keras.layers.Dropout( percentage_dropout ))\n",
    "\n",
    "modelb = Sequential()\n",
    "\n",
    "modelb.add(keras.layers.Merge (\n",
    "                               [model3, model4], \n",
    "                               mode = 'concat'\n",
    "                              ))\n",
    "\n",
    "modelb.add(keras.layers.Dense (\n",
    "                               units              = number_nodes * 2, \n",
    "                               kernel_initializer = weights, \n",
    "                               bias_initializer   = bias\n",
    "                              ))\n",
    "\n",
    "modelb.add(keras.layers.BatchNormalization())\n",
    "modelb.add(keras.layers.Activation('relu'))\n",
    "modelb.add(keras.layers.Dropout( percentage_dropout ))\n",
    "\n",
    "modelb.add(keras.layers.Dense (\n",
    "                               units              = number_nodes, \n",
    "                               kernel_initializer = weights, \n",
    "                               bias_initializer   = bias\n",
    "                              ))\n",
    "\n",
    "modelb.add(keras.layers.BatchNormalization())\n",
    "modelb.add(keras.layers.Activation('relu'))\n",
    "modelb.add(keras.layers.Dropout( percentage_dropout ))\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(keras.layers.Merge ( \n",
    "                              [modela, modelb], \n",
    "                              mode = 'concat'\n",
    "                             ))\n",
    "\n",
    "model.add(keras.layers.Dense (\n",
    "                              units              = number_nodes * 2, \n",
    "                              kernel_initializer = weights, \n",
    "                              bias_initializer   = bias\n",
    "                             ))\n",
    "\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Activation('relu'))\n",
    "model.add(keras.layers.Dropout( percentage_dropout ))\n",
    "\n",
    "model.add(keras.layers.Dense (\n",
    "                              units              = number_nodes, \n",
    "                              kernel_initializer = weights, \n",
    "                              bias_initializer   = bias\n",
    "                             ))\n",
    "\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Activation('relu'))\n",
    "model.add(keras.layers.Dropout( percentage_dropout ))\n",
    "\n",
    "model.add(keras.layers.Dense (\n",
    "                              units              = number_nodes, \n",
    "                              kernel_initializer = weights, \n",
    "                              bias_initializer   = bias\n",
    "                             ))\n",
    "\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Activation('relu'))\n",
    "model.add(keras.layers.Dropout( percentage_dropout ))\n",
    "\n",
    "model.add(keras.layers.Dense (\n",
    "                              units              = 1, \n",
    "                              kernel_initializer = weights, \n",
    "                              bias_initializer   = bias\n",
    "                             ))\n",
    "\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Activation('sigmoid'))\n",
    "\n",
    "model.compile (\n",
    "               loss      = 'binary_crossentropy', \n",
    "               optimizer = 'adam', \n",
    "               metrics   = ['accuracy']\n",
    "              )\n",
    "\n",
    "model.summary()\n",
    "\n",
    "####################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execute the model\n",
      "Train on 343646 samples, validate on 60644 samples\n",
      "Epoch 1/2\n",
      "343646/343646 [==============================] - 77s - loss: 0.5248 - acc: 0.7363 - val_loss: 0.4785 - val_acc: 0.7583\n",
      "Epoch 2/2\n",
      "343646/343646 [==============================] - 73s - loss: 0.4554 - acc: 0.7792 - val_loss: 0.4298 - val_acc: 0.7923\n",
      "\n",
      " Best Score: 0.429842555417\n"
     ]
    }
   ],
   "source": [
    "####################################################\n",
    "\n",
    "print 'Execute the model'\n",
    "\n",
    "CNN_best_weights = 'CNN_best_weights.h5'\n",
    "\n",
    "callbacks = [\n",
    "             keras.callbacks.ModelCheckpoint (\n",
    "                                              filepath       = CNN_best_weights, \n",
    "                                              monitor        = 'val_loss', \n",
    "                                              save_best_only = True\n",
    "                                             ),\n",
    "\n",
    "             keras.callbacks.EarlyStopping (\n",
    "                                            monitor  = 'val_loss', \n",
    "                                            patience = 5, \n",
    "                                            verbose  = 1, \n",
    "                                            mode     = 'auto'\n",
    "                                           )\n",
    "            ]\n",
    "\n",
    "hist = model.fit (\n",
    "                  x                = [training_data_1, training_data_2, training_data_1, training_data_2],\n",
    "                  y                = training_labels,\n",
    "                  batch_size       = 256,\n",
    "                  epochs           = 2,\n",
    "                  validation_split = 0.15,\n",
    "                  verbose          = True,\n",
    "                  shuffle          = True,\n",
    "                  callbacks        = callbacks\n",
    "                 )\n",
    "\n",
    "print '\\n Best Score:', min(hist.history['val_loss'])\n",
    "\n",
    "####################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary Statistics\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_acc</th>\n",
       "      <th>valid_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.736304</td>\n",
       "      <td>0.524847</td>\n",
       "      <td>0.758278</td>\n",
       "      <td>0.478542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.779250</td>\n",
       "      <td>0.455426</td>\n",
       "      <td>0.792313</td>\n",
       "      <td>0.429843</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   epoch  train_acc  train_loss  valid_acc  valid_loss\n",
       "0      1   0.736304    0.524847   0.758278    0.478542\n",
       "1      2   0.779250    0.455426   0.792313    0.429843"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "####################################################\n",
    "\n",
    "print 'Summary Statistics'\n",
    "\n",
    "# Aggregate the summary statistics\n",
    "summary_stats = pd.DataFrame({\n",
    "                              'epoch'      : [ i + 1 for i in hist.epoch ],\n",
    "                              'train_acc'  : hist.history['acc'],\n",
    "                              'valid_acc'  : hist.history['val_acc'],\n",
    "                              'train_loss' : hist.history['loss'],\n",
    "                              'valid_loss' : hist.history['val_loss']\n",
    "                            })\n",
    "summary_stats\n",
    "\n",
    "####################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary Plots\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAEACAYAAABbMHZzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc7d5dcf610>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "####################################################\n",
    "\n",
    "print 'Summary Plots'\n",
    "\n",
    "plt.plot(summary_stats.train_loss) # blue\n",
    "plt.plot(summary_stats.valid_loss) # green\n",
    "plt.show()\n",
    "\n",
    "####################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum Validation Loss During Training\n",
      "('Minimum loss at epoch', '2', '=', '0.4298')\n"
     ]
    }
   ],
   "source": [
    "####################################################\n",
    "\n",
    "print 'Minimum Validation Loss During Training'\n",
    "\n",
    "min_loss, idx = min( (loss, idx) for (idx, loss) in enumerate( hist.history['val_loss'] ))\n",
    "\n",
    "print('Minimum loss at epoch', '{:d}'.format(idx+1), '=', '{:.4f}'.format(min_loss))\n",
    "\n",
    "####################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save the predictions to a file for submission\n",
      "2345760/2345796 [============================>.] - ETA: 0sModel Saved\n"
     ]
    }
   ],
   "source": [
    "####################################################\n",
    "\n",
    "print 'Save the predictions to a file for submission'\n",
    "\n",
    "model.load_weights( CNN_best_weights )\n",
    "\n",
    "predictions = model.predict (\n",
    "                             x       = [\n",
    "                                        testing_data_1, \n",
    "                                        testing_data_2, \n",
    "                                        testing_data_1, \n",
    "                                        testing_data_2\n",
    "                                       ], \n",
    "                             verbose = True\n",
    "                            )\n",
    "\n",
    "submission = pd.DataFrame(predictions, columns=['is_duplicate'])\n",
    "submission.insert(0, 'test_id', testing_data_pd.test_id)\n",
    "submission.to_csv('CNN_submission_to_kaggle.csv', index=False)\n",
    "\n",
    "print 'Model Saved'\n",
    "\n",
    "####################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/pynb\n",
      "-rw-rw-r-- 1 ubuntu ubuntu 54538437 Aug 12 04:04 CNN_submission_to_kaggle.csv\n"
     ]
    }
   ],
   "source": [
    "!pwd\n",
    "!ls -l CNN_submission_to_kaggle.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
