{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import re\n",
    "import csv\n",
    "import nltk\n",
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The function text_to_wordlist is adapted from:\n",
    "# kaggle.com/currie32/quora-question-pairs/the-importance-of-cleaning-text\n",
    "\n",
    "def text_to_wordlist(\n",
    "                     question, \n",
    "                     remove_stopwords   = False, \n",
    "                     stem_words         = False,\n",
    "                     remove_punctuation = False\n",
    "                    ):\n",
    "\n",
    "    question = re.sub( r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \"          , question)\n",
    "    question = re.sub( r\"what's\"                , \"what is \"   , question)\n",
    "    question = re.sub( r\"\\'s\"                   , \" \"          , question)\n",
    "    question = re.sub( r\"\\'ve\"                  , \" have \"     , question)\n",
    "    question = re.sub( r\"can't\"                 , \"cannot \"    , question)\n",
    "    question = re.sub( r\"n't\"                   , \" not \"      , question)\n",
    "    question = re.sub( r\"i'm\"                   , \"i am \"      , question)\n",
    "    question = re.sub( r\"\\'re\"                  , \" are \"      , question)\n",
    "    question = re.sub( r\"\\'d\"                   , \" would \"    , question)\n",
    "    question = re.sub( r\"\\'ll\"                  , \" will \"     , question)\n",
    "    question = re.sub( r\",\"                     , \" \"          , question)\n",
    "    question = re.sub( r\"\\.\"                    , \" \"          , question)\n",
    "    question = re.sub( r\"!\"                     , \" ! \"        , question)\n",
    "    question = re.sub( r\"\\/\"                    , \" \"          , question)\n",
    "    question = re.sub( r\"\\^\"                    , \" ^ \"        , question)\n",
    "    question = re.sub( r\"\\+\"                    , \" + \"        , question)\n",
    "    question = re.sub( r\"\\-\"                    , \" - \"        , question)\n",
    "    question = re.sub( r\"\\=\"                    , \" = \"        , question)\n",
    "    question = re.sub( r\"'\"                     , \" \"          , question)\n",
    "    question = re.sub( r\"(\\d+)(k)\"              , r\"\\g<1>000\"  , question)\n",
    "    question = re.sub( r\":\"                     , \" : \"        , question)\n",
    "    question = re.sub( r\" e g \"                 , \" eg \"       , question)\n",
    "    question = re.sub( r\" b g \"                 , \" bg \"       , question)\n",
    "    question = re.sub( r\" u s \"                 , \" american \" , question)\n",
    "    question = re.sub( r\"\\0s\"                   , \"0\"          , question)\n",
    "    question = re.sub( r\" 9 11 \"                , \"911\"        , question)\n",
    "    question = re.sub( r\"e - mail\"              , \"email\"      , question)\n",
    "    question = re.sub( r\"j k\"                   , \"jk\"         , question)\n",
    "    question = re.sub( r\"\\s{2,}\"                , \" \"          , question)\n",
    "\n",
    "    if remove_stopwords:\n",
    "        question = question.lower().split()\n",
    "        question = [w for w in question if not w in nltk.corpus.stopwords.words(\"english\")]\n",
    "        question = ' '.join(question)\n",
    "\n",
    "    if stem_words:        \n",
    "        question      = question.lower().split()\n",
    "        stemmed_words = [nltk.stem.SnowballStemmer('english').stemmer.stem(word) for word in question]\n",
    "        question      = ' '.join(stemmed_words)\n",
    "\n",
    "    if remove_punctuation:\n",
    "        question = ''.join([character for character in question if character not in string.punctuation])\n",
    "    \n",
    "    return(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_data_pd = pd.read_csv( '/home/ubuntu/train.csv' )\n",
    "testing_data_pd  = pd.read_csv( '/home/ubuntu/test.csv'  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing training dataset\n",
      "Found 404290 question pairs in train.csv\n",
      "Processing test dataset\n",
      "Found 2345796 question pairs in test.csv\n"
     ]
    }
   ],
   "source": [
    "####################################################\n",
    "\n",
    "print 'Processing training dataset'\n",
    "\n",
    "training_questions_1 = [] \n",
    "training_questions_2 = []\n",
    "training_labels      = []\n",
    "\n",
    "with open( '/home/ubuntu/train.csv' ) as f:\n",
    "    \n",
    "    reader = csv.reader(f, delimiter=',')\n",
    "    header = next(reader)\n",
    "    \n",
    "    for row in reader:\n",
    "        \n",
    "        training_questions_1.append( text_to_wordlist ( row[3] ))\n",
    "        training_questions_2.append( text_to_wordlist ( row[4] ))\n",
    "        training_labels.append( int(row[5] ))\n",
    "        \n",
    "print 'Found %s question pairs in train.csv' % len(training_questions_1)\n",
    "\n",
    "####################################################\n",
    "\n",
    "print 'Processing test dataset'\n",
    "\n",
    "test_questions_1  = []\n",
    "test_questions_2  = []\n",
    "test_question_ids = []\n",
    "\n",
    "with open( '/home/ubuntu/test.csv' ) as f:\n",
    "    \n",
    "    reader = csv.reader(f, delimiter=',')\n",
    "    header = next(reader)\n",
    "    \n",
    "    for row in reader:\n",
    "        \n",
    "        test_questions_1.append( text_to_wordlist ( row[1] ))\n",
    "        test_questions_2.append( text_to_wordlist ( row[2] ))\n",
    "        test_question_ids.append( row[0] )\n",
    "        \n",
    "print 'Found %s question pairs in test.csv' % len(test_questions_1)\n",
    "\n",
    "####################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing Words with KERAS\n",
      "Found 120539 unique word tokens\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "####################################################\n",
    "\n",
    "print 'Tokenizing Words with KERAS'\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts ( \n",
    "                        training_questions_1 + \n",
    "                        training_questions_2 + \n",
    "                        test_questions_1  +\n",
    "                        test_questions_2\n",
    "                       )\n",
    "\n",
    "training_sequences_1 = tokenizer.texts_to_sequences( training_questions_1 )\n",
    "training_sequences_2 = tokenizer.texts_to_sequences( training_questions_2 )\n",
    "testing_sequences_1  = tokenizer.texts_to_sequences( test_questions_1 )\n",
    "testing_sequences_2  = tokenizer.texts_to_sequences( test_questions_2 )\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "print 'Found %s unique word tokens' % len(word_index)\n",
    "\n",
    "####################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Tensors with KERAS\n",
      "Shape of training data tensor: (404290, 20)\n",
      "Shape of testing data tensor: (2345796, 20)\n",
      "Number of labels: 404290\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "maximum_sequence_length = 20 # average sentence length from EDA\n",
    "\n",
    "####################################################\n",
    "\n",
    "print 'Creating Tensors with KERAS'\n",
    "\n",
    "training_data_1   = pad_sequences (\n",
    "                                   sequences  = training_sequences_1, \n",
    "                                   maxlen     = maximum_sequence_length\n",
    "                                  )\n",
    "\n",
    "training_data_2   = pad_sequences (\n",
    "                                   sequences  = training_sequences_2, \n",
    "                                   maxlen     = maximum_sequence_length \n",
    "                                  )\n",
    "\n",
    "testing_data_1    = pad_sequences (\n",
    "                                   sequences  = testing_sequences_1, \n",
    "                                   maxlen     = maximum_sequence_length\n",
    "                                  )\n",
    "\n",
    "testing_data_2    = pad_sequences ( \n",
    "                                   sequences  = testing_sequences_2, \n",
    "                                   maxlen     = maximum_sequence_length\n",
    "                                  )\n",
    "\n",
    "training_labels   = training_data_pd.is_duplicate\n",
    "\n",
    "print 'Shape of training data tensor:',  training_data_1.shape\n",
    "print 'Shape of testing data tensor:',   testing_data_1.shape\n",
    "print 'Number of labels:',               len(training_labels)\n",
    "\n",
    "####################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors\n",
      "Found 2196017 word vectors of word2vec\n"
     ]
    }
   ],
   "source": [
    "####################################################\n",
    "\n",
    "print 'Indexing word vectors'\n",
    "\n",
    "embeddings_index = {}\n",
    "\n",
    "with open( '/home/ubuntu/glove.840B.300d.txt' ) as f:\n",
    "\n",
    "    for line in f:\n",
    "\n",
    "        word_embeddings        = line.split(' ')\n",
    "        word                   = word_embeddings[0]\n",
    "        embedding              = np.asarray( word_embeddings[1:], dtype='float32' )\n",
    "        embeddings_index[word] = embedding\n",
    "\n",
    "print 'Found %s word vectors of word2vec' % len(embeddings_index)\n",
    "\n",
    "####################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing embedding matrix\n",
      "Embedding Matrix Shape: (120540, 300)\n"
     ]
    }
   ],
   "source": [
    "embedding_dimension = 300  # size of word vectors from GloVe\n",
    "\n",
    "####################################################\n",
    "\n",
    "print 'Preparing embedding matrix'\n",
    "\n",
    "number_of_words = len(word_index)\n",
    "\n",
    "embedding_matrix = np.zeros( (number_of_words + 1, embedding_dimension) )\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    \n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    \n",
    "    if embedding_vector is not None:\n",
    "        \n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "print 'Embedding Matrix Shape:', embedding_matrix.shape\n",
    "\n",
    "####################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared model parameters\n"
     ]
    }
   ],
   "source": [
    "####################################################\n",
    "\n",
    "print 'Prepared model parameters'\n",
    "\n",
    "number_nodes       = 256  # Number of nodes in the Dense layers\n",
    "percentage_dropout = 0.05 # Percentage of nodes to drop for regularization and generalization\n",
    "number_filters     = 32   # Number of filters to use in Convolution1D\n",
    "filter_length      = 6    # Length of filter for Convolution1D\n",
    "\n",
    "weights            = keras.initializers.TruncatedNormal (\n",
    "                                                         mean   = 0.0, \n",
    "                                                         stddev = 0.05, \n",
    "                                                         seed   = 2\n",
    "                                                        )\n",
    "bias               = bias_initializer = 'zeros'\n",
    "\n",
    "####################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defined the model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/IPython/kernel/__main__.py:113: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/usr/local/lib/python2.7/dist-packages/IPython/kernel/__main__.py:140: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/usr/local/lib/python2.7/dist-packages/IPython/kernel/__main__.py:167: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "merge_3 (Merge)              (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1)                 257       \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 1)                 4         \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 146,655,645\n",
      "Trainable params: 2,001,067\n",
      "Non-trainable params: 144,654,578\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "\n",
    "####################################################\n",
    "\n",
    "print 'Defined the model'\n",
    "\n",
    "model1 = Sequential()\n",
    "\n",
    "model1.add(keras.layers.Embedding (\n",
    "                                   input_dim    = number_of_words + 1,\n",
    "                                   output_dim   = embedding_dimension,\n",
    "                                   weights      = [embedding_matrix],\n",
    "                                   input_length = maximum_sequence_length,\n",
    "                                   trainable    = False\n",
    "                                  ))\n",
    "\n",
    "model1.add(keras.layers.Convolution1D (\n",
    "                                       filters     = number_filters, \n",
    "                                       kernel_size = filter_length, \n",
    "                                       padding     = 'same'\n",
    "                                      ))\n",
    "\n",
    "model1.add(keras.layers.BatchNormalization())\n",
    "model1.add(keras.layers.Activation( 'relu' ))\n",
    "model1.add(keras.layers.Dropout( percentage_dropout ))\n",
    "\n",
    "model1.add(keras.layers.Convolution1D (\n",
    "                                       filters     = number_filters, \n",
    "                                       kernel_size = filter_length, \n",
    "                                       padding     = 'same'\n",
    "                                      ))\n",
    "\n",
    "model1.add(keras.layers.BatchNormalization())\n",
    "model1.add(keras.layers.Activation('relu'))\n",
    "model1.add(keras.layers.Dropout( percentage_dropout ))\n",
    "\n",
    "model1.add(keras.layers.Flatten())\n",
    "\n",
    "model2 = Sequential()\n",
    "\n",
    "model2.add(keras.layers.Embedding (\n",
    "                                   input_dim    = number_of_words + 1,\n",
    "                                   output_dim   = embedding_dimension,\n",
    "                                   weights      = [embedding_matrix],\n",
    "                                   input_length = maximum_sequence_length,\n",
    "                                   trainable    = False\n",
    "                                  ))\n",
    "\n",
    "model2.add(keras.layers.Convolution1D (\n",
    "                                       filters     = number_filters, \n",
    "                                       kernel_size = filter_length, \n",
    "                                       padding     = 'same'\n",
    "                                      ))\n",
    "\n",
    "model2.add(keras.layers.BatchNormalization())\n",
    "model2.add(keras.layers.Activation('relu'))\n",
    "model2.add(keras.layers.Dropout( percentage_dropout ))\n",
    "\n",
    "model2.add(keras.layers.Convolution1D (\n",
    "                                       filters     = number_filters, \n",
    "                                       kernel_size = filter_length, \n",
    "                                       padding     = 'same'\n",
    "                                      ))\n",
    "\n",
    "model2.add(keras.layers.BatchNormalization())\n",
    "model2.add(keras.layers.Activation('relu'))\n",
    "model2.add(keras.layers.Dropout( percentage_dropout ))\n",
    "\n",
    "model2.add(keras.layers.Flatten())\n",
    "\n",
    "model3 = Sequential()\n",
    "\n",
    "model3.add(keras.layers.Embedding (\n",
    "                                   input_dim    = number_of_words + 1,\n",
    "                                   output_dim   = embedding_dimension,\n",
    "                                   weights      = [embedding_matrix],\n",
    "                                   input_length = maximum_sequence_length,\n",
    "                                   trainable    = False\n",
    "                                  ))\n",
    "\n",
    "model3.add(keras.layers.TimeDistributed(keras.layers.Dense( embedding_dimension )))\n",
    "model3.add(keras.layers.BatchNormalization())\n",
    "model3.add(keras.layers.Activation('relu'))\n",
    "model3.add(keras.layers.Dropout( percentage_dropout ))\n",
    "model3.add(keras.layers.Lambda (\n",
    "                                lambda x: keras.backend.max(x, axis=1), \n",
    "                                output_shape = ( embedding_dimension, )\n",
    "                               ))\n",
    "\n",
    "model4 = Sequential()\n",
    "\n",
    "model4.add(keras.layers.Embedding (\n",
    "                                   input_dim    = number_of_words + 1,\n",
    "                                   output_dim   = embedding_dimension,\n",
    "                                   weights      = [embedding_matrix],\n",
    "                                   input_length = maximum_sequence_length,\n",
    "                                   trainable    = False\n",
    "                                  ))\n",
    "\n",
    "model4.add(keras.layers.TimeDistributed(keras.layers.Dense( embedding_dimension )))\n",
    "model4.add(keras.layers.BatchNormalization())\n",
    "model4.add(keras.layers.Activation('relu'))\n",
    "model4.add(keras.layers.Dropout( percentage_dropout ))\n",
    "model4.add(keras.layers.Lambda (\n",
    "                                lambda x: keras.backend.max(x, axis=1), \n",
    "                                output_shape = ( embedding_dimension, )\n",
    "                               ))\n",
    "\n",
    "modela = Sequential()\n",
    "\n",
    "modela.add(keras.layers.Merge (\n",
    "                               [model1, model2], \n",
    "                               mode = 'concat'\n",
    "                              ))\n",
    "\n",
    "modela.add(keras.layers.Dense (\n",
    "                               units              = number_nodes * 2, \n",
    "                               kernel_initializer = weights, \n",
    "                               bias_initializer   = bias\n",
    "                              ))\n",
    "\n",
    "modela.add(keras.layers.BatchNormalization())\n",
    "modela.add(keras.layers.Activation('relu'))\n",
    "modela.add(keras.layers.Dropout(percentage_dropout))\n",
    "\n",
    "modela.add(keras.layers.Dense (\n",
    "                               units              = number_nodes, \n",
    "                               kernel_initializer = weights, \n",
    "                               bias_initializer   = bias\n",
    "                              ))\n",
    "\n",
    "modela.add(keras.layers.BatchNormalization())\n",
    "modela.add(keras.layers.Activation('relu'))\n",
    "modela.add(keras.layers.Dropout( percentage_dropout ))\n",
    "\n",
    "modelb = Sequential()\n",
    "\n",
    "modelb.add(keras.layers.Merge (\n",
    "                               [model3, model4], \n",
    "                               mode = 'concat'\n",
    "                              ))\n",
    "\n",
    "modelb.add(keras.layers.Dense (\n",
    "                               units              = number_nodes * 2, \n",
    "                               kernel_initializer = weights, \n",
    "                               bias_initializer   = bias\n",
    "                              ))\n",
    "\n",
    "modelb.add(keras.layers.BatchNormalization())\n",
    "modelb.add(keras.layers.Activation('relu'))\n",
    "modelb.add(keras.layers.Dropout( percentage_dropout ))\n",
    "\n",
    "modelb.add(keras.layers.Dense (\n",
    "                               units              = number_nodes, \n",
    "                               kernel_initializer = weights, \n",
    "                               bias_initializer   = bias\n",
    "                              ))\n",
    "\n",
    "modelb.add(keras.layers.BatchNormalization())\n",
    "modelb.add(keras.layers.Activation('relu'))\n",
    "modelb.add(keras.layers.Dropout( percentage_dropout ))\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(keras.layers.Merge ( \n",
    "                              [modela, modelb], \n",
    "                              mode = 'concat'\n",
    "                             ))\n",
    "\n",
    "model.add(keras.layers.Dense (\n",
    "                              units              = number_nodes * 2, \n",
    "                              kernel_initializer = weights, \n",
    "                              bias_initializer   = bias\n",
    "                             ))\n",
    "\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Activation('relu'))\n",
    "model.add(keras.layers.Dropout( percentage_dropout ))\n",
    "\n",
    "model.add(keras.layers.Dense (\n",
    "                              units              = number_nodes, \n",
    "                              kernel_initializer = weights, \n",
    "                              bias_initializer   = bias\n",
    "                             ))\n",
    "\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Activation('relu'))\n",
    "model.add(keras.layers.Dropout( percentage_dropout ))\n",
    "\n",
    "model.add(keras.layers.Dense (\n",
    "                              units              = number_nodes, \n",
    "                              kernel_initializer = weights, \n",
    "                              bias_initializer   = bias\n",
    "                             ))\n",
    "\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Activation('relu'))\n",
    "model.add(keras.layers.Dropout( percentage_dropout ))\n",
    "\n",
    "model.add(keras.layers.Dense (\n",
    "                              units              = 1, \n",
    "                              kernel_initializer = weights, \n",
    "                              bias_initializer   = bias\n",
    "                             ))\n",
    "\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Activation('sigmoid'))\n",
    "\n",
    "model.compile (\n",
    "               loss      = 'binary_crossentropy', \n",
    "               optimizer = 'adam', \n",
    "               metrics   = ['accuracy']\n",
    "              )\n",
    "\n",
    "model.summary()\n",
    "\n",
    "####################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execute the model\n",
      "Train on 323432 samples, validate on 80858 samples\n",
      "Epoch 1/5\n",
      "323432/323432 [==============================] - 101s - loss: 0.4867 - acc: 0.7603 - val_loss: 0.4519 - val_acc: 0.7828\n",
      "Epoch 2/5\n",
      "323432/323432 [==============================] - 98s - loss: 0.4108 - acc: 0.8051 - val_loss: 0.4078 - val_acc: 0.8077\n",
      "Epoch 3/5\n",
      "323432/323432 [==============================] - 98s - loss: 0.3707 - acc: 0.8279 - val_loss: 0.3863 - val_acc: 0.8206\n",
      "Epoch 4/5\n",
      "323432/323432 [==============================] - 98s - loss: 0.3372 - acc: 0.8473 - val_loss: 0.3871 - val_acc: 0.8249\n",
      "Epoch 5/5\n",
      "323432/323432 [==============================] - 98s - loss: 0.3054 - acc: 0.8641 - val_loss: 0.3878 - val_acc: 0.8250\n",
      "\n",
      " Best Score: 0.386277519125\n"
     ]
    }
   ],
   "source": [
    "####################################################\n",
    "\n",
    "print 'Execute the model'\n",
    "\n",
    "CNN_best_weights = 'CNN_best_weights.h5'\n",
    "\n",
    "callbacks = [\n",
    "             keras.callbacks.ModelCheckpoint (\n",
    "                                              filepath       = CNN_best_weights, \n",
    "                                              monitor        = 'val_loss', \n",
    "                                              save_best_only = True\n",
    "                                             ),\n",
    "\n",
    "             keras.callbacks.EarlyStopping (\n",
    "                                            monitor  = 'val_loss', \n",
    "                                            patience = 3, \n",
    "                                            verbose  = 1, \n",
    "                                            mode     = 'auto'\n",
    "                                           )\n",
    "            ]\n",
    "\n",
    "hist = model.fit (\n",
    "                  x                = [training_data_1, training_data_2, training_data_1, training_data_2],\n",
    "                  y                = training_labels,\n",
    "                  batch_size       = 128,\n",
    "                  epochs           = 5,\n",
    "                  validation_split = 0.20,\n",
    "                  verbose          = True,\n",
    "                  shuffle          = True,\n",
    "                  callbacks        = callbacks\n",
    "                 )\n",
    "\n",
    "print '\\n Best Score:', min(hist.history['val_loss'])\n",
    "\n",
    "####################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary Statistics\n"
     ]
    }
   ],
   "source": [
    "####################################################\n",
    "\n",
    "print 'Summary Statistics'\n",
    "\n",
    "# Aggregate the summary statistics\n",
    "summary_stats = pd.DataFrame({\n",
    "                              'epoch'      : [ i + 1 for i in hist.epoch ],\n",
    "                              'train_acc'  : hist.history['acc'],\n",
    "                              'valid_acc'  : hist.history['val_acc'],\n",
    "                              'train_loss' : hist.history['loss'],\n",
    "                              'valid_loss' : hist.history['val_loss']\n",
    "                            })\n",
    "\n",
    "####################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary Plots\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAEACAYAAABbMHZzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xl4FGW2x/HvSdgUFFQEFASEoIDI4IaMiEQgQFBEHVRU\nBhQdQVZBZ9yuo47K4KgwEsU77rvoqKMgCbLGFRVZLmtYXRBXVtmX5L1/vG0mxMR0QqerO/37PA8P\nqe6q6pOCPlV16l3MOYeIiCSWpKADEBGR6FPyFxFJQEr+IiIJSMlfRCQBKfmLiCQgJX8RkQQUVvI3\ns+5mlmNmK83s5iLe729mP5rZ/NCfAYXeW2lmK8ysXySDFxGRsrGS2vmbWRKwEugMfAvMBfo453IK\nrNMfOM05N7zQtkcAnwOnAgbMA051zm2N5C8hIiKlE86Vf1tglXPuK+fcPmAi0KuI9ayI17oB05xz\nW51zW4BpQPcyRysiIhERTvKvD6wrsPxN6LXCLjazhWb2mpn98n7hbdcXs62IiERROMm/qCv6wrWi\nSUBj51wbYCbwfCm2FRGRKKsUxjrfAA0LLDfA1/7zOec2F1h8AhhTYNvUQtvOLvwBZqYTgohIGTjn\nirrILlE4V/5zgRQza2RmVYA++Cv9fGZWr8BiL2B56Od3gTQzqxl6+JsWeu1XnHMx/+fOO+8MPAbF\nqTjjOc54iDGe4jwYJV75O+dyzWwo/mFtEvCUc265md0NzHXOvQMMN7MLgH3AJuCq0LabzewefIsf\nB9zt/INfEREJUDhlH5xzU4ETC712Z4GfbwNuK2bbZ4FnyxyhiIhEnHr4lkJqamrQIYRFcUaW4oyc\neIgR4ifOg1FiJ6+oBGHmYiEOEZF4Yma4cnzgKyIiFYySv4hIAlLyFxFJQEr+IiIJSMlfRCQBKfmL\niCQgJX8RkQSk5C8ikoBiJvkvWRJ0BCIiiSNmkn/XrrBsWdBRiIgkhphJ/v/4B6SlQU5OyeuKiMjB\nCWtUz2jo2xdyc6FLF5g1C044IeiIREQqrphJ/gD9+/sTQOfO/gTQrFnQEYmIVEwxlfwBBgyAvDx/\nApg9G5o2DToiEZGKJ+aSP8C11/o7gE6dIDsbjj8+6IhERCqWmEz+AAMH/vcEMHs2NG4cdEQiIhVH\nzCZ/gMGDD7wDaNgw6IhERCqGmE7+AMOG+RPAuef6E8BxxwUdkYhI/Iv55A9www0H3gHUrx90RCIi\n8S0ukj/AjTceeAdw7LFBRyQiEr/iJvkD/OUvB94B1KsXdEQiIvEprpI/wK23HtgKqG7doCMSEYk/\ncZf8Af7nfw7sCVynTtARiYjEl7hM/gB//Svs3//fsYBq1w46IhGR+BG3yd8M/va3A+8Ajjoq6KhE\nROJD3CZ/8CeA++7772igM2fCkUcGHZWISOyLmfH8y8oMxozxV/9pabB5c9ARiYjEvrhP/uBPAA88\nAOecA926wZYtQUckIhLbKkTyB38CGDsW2rWD7t3h55+DjkhEJHZVmOQP/gTw8MNw2mn+BLBtW9AR\niYjEpgqV/MGfADIyoHVrSE/XCUBEpCgVLvkDJCXBhAnQogWcdx7s2BF0RCIisaVCJn/wJ4B//QtS\nUuD882HnzqAjEhGJHRU2+YM/ATzxhJ8EpmdPnQBERH5RoZM/QHIyPP20HwK6Vy/YtSvoiEREglfh\nkz/4E8Czz8LRR8OFF8Lu3UFHJCISrIRI/uBPAM8/D0ccARdfDHv2BB2RiEhwEib5A1SqBC++CNWr\nwx/+oBOAiCSuhEr+4E8AL78MVarApZfC3r1BRyQiEn0Jl/wBKleGiRN9h7A+fWDfvqAjEhGJroRM\n/uCv/F97zSf+yy/XCUBEEkvCJn/wJ4DXX/fNP6+80s8MJiKSCBI6+QNUrQpvvOHHAPrjH3UCEJHE\nkPDJH6BaNXjzTdi4Ea66ys8MJiJSkSn5hxxyCLz1Fnz3HQwYoBOAiFRsYSV/M+tuZjlmttLMbv6N\n9XqbWZ6ZnRpabmRmO81sfujPhEgFXh4OPRQmT4avv4Zrr4W8vKAjEhEpH+ac++0VzJKAlUBn4Ftg\nLtDHOZdTaL0awBSgMjDUOTffzBoBk51zrUv4DFdSHNG0Ywf06AHNmsHjj/sB4kREYo2Z4Zyzsmwb\nTlprC6xyzn3lnNsHTAR6FbHePcD9QOF+s2EFNuidQWzYuSGcVctd9eowZQqsWAHXX687ABGpeMJJ\n/vWBdQWWvwm9ls/M2gANnHOZRWzf2MzmmdlsMzu7uA+pmlyVlo+25NHPHmV/XvBNbmrUgMxMWLwY\nhg6FGLoxERE5aJXCWKeoK/f8VGhmBowD+hexzXdAQ+fc5tBzgLfMrKVzbnvhHR7x6RH03tGbB0c/\nyNgmY3l6+NN0bNwx/N+kHBx2GEydCl27wvDhMH687xUsIhKE7OxssrOzI7KvcGr+7YC7nHPdQ8u3\nAM45d39o+XBgNbAdn/TrARuBC5xz8wvtazZwYxGv59f8nXO8sfwNbpx2I79v8HseSHuA42oeF4Ff\ntey2boW0NDjrLBg3TicAEYkN5V3znwukhFruVAH6AJN+edM597Nzro5zrolz7njgE6Bn6IFv7dAD\nY8ysCZACrC3pl+ndsjfLhyznxKNO5JR/ncJ979/H7v3BDcJfsyZMmwYffgg33aQSkIjEvxKTv3Mu\nFxgKTAOWAhOdc8vN7G4zO7+oTfhv2eccYJGZLQBeAwY657aEE9ihlQ/l7nPvZu6f5jL/+/mcNOEk\nJq2YRFCtgmrV8ieA2bPh5pt1AhCR+FZi2ScqQYTR1HP6mumMmDqChjUb8s/u/6R57eZRiu5AGzdC\n586Qng6jR6sEJCLBKe+yT0xIa5rG/w36P7o17UaHZzrw52l/5uc9P0c9jqOOghkzfFPQO+7QHYCI\nxKe4Sf4AlZMrM/L3I1ly/RI27tpIi0db8Pz/PU+ei25D/Nq1YeZMPxzE3XdH9aNFRCIibso+Rfls\n/WcMzRxKpaRKjE8fz+nHnl4O0RXvhx/g3HP9fAB33BHVjxYROaiyT1wnf4A8l8dzC5/jtlm3cX6z\n8xndeTRHVz86whEW7/vv/Qngj3+E226L2seKiCRGzb84SZbE1adcTc6QHA6rehgtJ7Rk/Kfjo9ZL\nuF49mDULnnsO7r8/Kh8pInLQ4v7Kv7BlPy1jxNQRfLftO8anj6fT8Z0ist+SrF8PqakwcKDvCyAi\nUt4SuuxTFOccb+W8xahpozj92NN5qOtDNKzZMGL7L84330DHjn4soJEjy/3jRCTBJXTZpyhmxkUt\nLmLZ4GWcXOdkTv3Xqfztvb+xa9+ucv3cBg18J7CMDD8OkIhIrKqQV/6FfbXlK26afhOff/s5Y7uO\n5cLmF2Ll2Dvrq698Ceimm2DIkHL7GBFJcCr7hGnWF7MYnjWcYw47hvHdx9Pi6Bbl9llffOFbAd1y\nCwwaVG4fIyIJTGWfMHU6vhMLBi6g5wk9OefZcxj17ii27t5aLp91/PG+I9jo0X42MBGRWJJQyR98\nL+HhZw5n6eClbNuzjeaPNueZBc+USy/hpk19M9B77oGnnor47kVEyiyhyj5Fmbt+LsOnDifP5ZGR\nnkHb+m0j/hkrV0KnTnDvvXDVVRHfvYgkKNX8D1Key+PFRS9yy4xbSE9JZ3Tn0dStUTein5GT40cD\nHTPG9wYWETlYqvkfpCRLot/v+pEzNIcjDzmSVo+1YtyccezL3Rexz2je3I8GevPN8PLLEdutiEiZ\n6Mq/CDkbchgxdQTrtq5jfPp4ujTpErF9L10KXbr46SD79InYbkUkAansUw6cc0xeOZmR746kTb02\nPNT1IRrXahyRfS9e7OcEzsiASy6JyC5FJAGp7FMOzIwLTryApYOXcmq9Uzn98dO5c/ad7Ny386D3\nffLJ8O67MGwYvPFGBIIVESklJf8SVKtUjdvPuZ0FAxewYuMKWjzagteXvX7Qcwn/7neQlQWDB/tJ\nYUREoklln1LK/jKb4VnDObr60YzvPp6T6px0UPubNw969IAnn4SePSMUpIgkBJV9oii1cSrzB87n\nouYXce5z5zIiawRbdm8p8/5OOw3eeQeuucbPCywiEg1K/mVQKakSQ9sOZdmQZezJ3UPzR5rz5Pwn\ny9xL+IwzYPJkuPpqXwoSESlvKvtEwPzv5jMsaxh7c/eSkZ5BuwbtyrSfjz+GXr3gxRehW7cIByki\nFY6aesYA5xwvLX6Jm2fcTFqTNMZ0GUO9GvVKvZ8PP4SLLoJXXvH9AUREiqOafwwwM/q27kvOkBzq\n1ahHqwmtePDjB9mbu7dU+zn7bN/88/LL/aBwIiLlQVf+5WTlxpXcMPUG1m5ey8PdH6ZbSunqONnZ\nvgPY66/7qSFFRApT2SdGOeeYsmoKN0y9gVZ1WjG221iaHNEk7O1nzYLLLoM334QOHcoxUBGJSyr7\nxCgz4/wTzmfp4KWcWf9M2j7Rljtm3cGOvTvC2r5TJz8I3MUXw0cflXOwIpJQlPyjoGqlqtza4VYW\nDlrIms1raPFoC15b+lpYvYTT0nzrnwsvhDlzohCsiCQElX0C8MFXHzAsaxi1qtVifPp4WtdtXeI2\nWVnQv7/vD3DmmVEIUkRinso+caZDow7Mu24el510GWkvpDEscxibdm36zW3S0+Hpp+GCC+Dzz6MU\nqIhUWEr+AUlOSub6M65n2eBl5LpcWjzagsfnPU5uXm6x25x/PjzxBJx3HsyfH8VgRaTCUdknRiz8\nfiHDs4azfe92MtIzaN+wfbHr/uc/MGgQTJ0Kp5wSxSBFJKaoqWcF4Zxj4pKJ/GXGX0htnMr9Xe7n\n2MOOLXLdN96AIUP8vAC/+12UAxWRmKCafwVhZlx+8uUsH7Kc4w4/jtaPteYfH/2DPfv3/GrdP/wB\nxo/3YwAtXhxAsCIS13TlH8NWb1rNyHdHsmLDCh7u/jDpzdJ/tc7EiTBypJ8c/qSDm1pAROKMyj4V\nXOaqTG6YegMn1j6Rcd3GkXJkygHvv/QS/PnPMHMmtGgRUJAiEnUq+1RwPZr1YMngJXRo2IF2T7bj\ntpm3sX3v9vz3r7wSxozxo4CuWBFgoCISN5T840SV5Cr8pf1fWHT9Itb9vI4Wj7bglcWv5PcS7tcP\n7r0XOneGpUsDDlZEYp7KPnHqo68/YljWMGpUqUFGega/q+eb/Dz/PIwa5UcE/etf4ZhjAg5URMqN\nyj4JqH3D9sz901z6tu5L1xe7MnjKYDbu3Ei/fr70U706tGoFt98OW7cGHa2IxBol/ziWnJTMdadd\nx/Ihy0m2ZFpOaMljcx+j1hG5PPggLFgA330HzZrBQw/B7t1BRywisUJlnwpk0Q+LGDF1BDkbcuie\n0p0eKT1Ia5rGt2trcfvtMG8e3HWXfz5QqVLQ0YrIwVJTTznA2s1ryVqVRebqTD746gPa1GtDeko6\nDXb14MnRrfnpR+O++/ww0Vam/zYiEguU/KVYu/bt4r2v3iNzVSaZqzLZtX8Xraqls+KddI7e1oUH\n762paSJF4pSSv4Rt1cZVZK3OYsqqTN7/4mP49lQa7unBvVel07tDK0y3AiJxQ8lfymTnvp1MWzWb\nsZOy+PinTKoeso+eLdLpc1oPOh/fmcOqHhZ0iCLyG8o9+ZtZd+Cf+NZBTznn7i9mvd7Aa8Dpzrn5\nodduBQYA+4ERzrlpRWyn5B+wbdsct49dyVPvZ3JUuyw2VZ/DmQ3akp6STo9mPWhRu4XuCkRiTLkm\nfzNLAlYCnYFvgblAH+dcTqH1agBTgMrAUOfcfDNrAbwMnAE0AGYAzQpneiX/2PHTTzB6NDz3yna6\nXjeb6qdkMuOrTAB6pPQgvVk6nY7vRI0qNQKOVETKu5NXW2CVc+4r59w+YCLQq4j17gHuBwqOP9wL\nmOic2++c+xJYFdqfxKijj4Zx42DBpzWo9nVP3hn0GCPyvuSt3lk0PbIpD3/6MMc8dAxpL6Qxbs44\ncjbkhDURvYjElnCSf31gXYHlb0Kv5TOzNkAD51xmCduuL7ytxKZGjeDZZ/1Ioe+9Z1zYviW1V97E\ntCtn8u2obxlyxhCWb1hO2gtpNB3flCFThjBl5RR27tsZdOgiEoZwuvoUdUuRf6lnvhA8Duhf2m0L\nuuuuu/J/Tk1NJTU1NYzQpLy1agVvvw0ffQS33AIPPgijRx9Gr54XcmHzC3HOseTHJWStzuKBjx+g\nzxt9aH9ce3o060F6SjrNjmoW9K8gUmFkZ2eTnZ0dkX2FU/NvB9zlnOseWr4FcL889DWzw4HVwHZ8\nsq8HbAQuALriVx4TWncqcKdz7tNCn6GafxxwDqZMgVtvhcMP98NId+hw4Dpbd29lxtoZZK3OInNV\nJtWrVCc9JZ30lHRSG6dySOVDgglepAIq7we+ycAK/APf74DPgMudc8uLWX82MMo5t8DMWgIvAWfi\nyz3T0QPfuJebCy+/7EcNPekk/4C4detfr+ecY9EPi8hclUnW6iwWfr+QDo065LcganJEk+gHL1KB\nRKup58P8t6nnGDO7G5jrnHun0LqzgJsKNfW8BtiHmnpWKHv2wP/+L/z975CWBn/7Gxx/fPHrb9m9\nhelrppO5OpOsVVnUrFYzvwXROY3OoVqlatELXqQCUCcvCdS2bX7U0IwM6NvXDyNdp85vb5Pn8lj4\n/cL8u4LFPyymY+OO+SeDxrUaRyV2kXim5C8x4ccf4b774MUXYdgwuPFGOCzMTsKbdm1i2pppZK7K\nZOrqqdQ+tHZ+eejshmdTtVLV8g1eJA4p+UtM+eIL/zxg+nS47TYYOBCqliJ357k85n07L/+h8fIN\nyzm38bn+wXGzdBrWbFh+wYvEESV/iUmLFvmWQcuW+ecBV1wBycml38+GnRt4d/W7ZK3OYurqqdSr\nUS+/KWn7hu2pklwl8sGLxAElf4lp77/v+whs3+4fDvfoUfZ5BHLzcvn828/znxWs3LiSTsd3yj8Z\n1D9cfQglcSj5S8xzDiZP9ncCRx4J998PZ5118Pv9ccePvLv6XTJXZzJtzTQaHN4g/1nB7xv8nsrJ\nlQ/+Q0RilJK/xI3cXHjhBbjzTmjTxj8gbtUqMvven7efz9Z/ln9XsHbzWro06UJ6SjrdU7pz7GHH\nRuaDRGKEkr/End274bHHfC/h9HS4+24/nlAkfb/9e6aunkrmqkymr51O41qN85uStmvQjkpJmshY\n4puSv8StrVv9eEETJviJ5W+/HWrXjvzn7M/bz5x1c/JbEH299WvSmqbRI6UH3VO6U7dG3ch/qEg5\nU/KXuPf993DvvTBxIowYASNHQo1ynDJg/c/rmbp6Klmrs5ixdgYpR6bkPytoW78tyUllaJYkEZPn\n8shzeeTm5fq/XW6Ry+GsU9rlmNlnCevluTyy+mYp+UvFsGYN3HEHzJ7t7wKuuw6qlHNLzn25+/h4\n3cf5zwrWb1tPt6bd8p8V1D609q++fLkuN//vg3mt4Bc7Uq+VOaYI/U4l/Z7hJD6AZEsmyZJITgr9\nXcxyOOuUdjmsbUq570jG98vyeSecp+QvFcvChb5l0MqVcM890KcPJIUz+0QErNu6zj8rWJ3JzLUz\n2bZ3G+CTUXJS8gFfwJJeK/ilLem1Uu032p8X4f2WlOA0ZWh4VPaRCis72/cR2L3b9xHo3r3sfQTK\n4per0CSL0plHpBSU/KVCcw7eessPFVG3rm8h1K5d0FGJBK+85/AVCZQZXHQRLF4Mf/wjXHKJX15e\n5IwSIhIOJX+JG5UqwTXX+OcA7dtDx44wYACsW1fytiJyICV/iTuHHAI33eRPAvXq+Z7CN90EGzcG\nHZlI/FDyl7hVq5afQnLJEtixA0480Q8XsWNH0JGJxD4lf4l7xxzjh4qYM8c/F2jWzC/v2xd0ZCKx\nS8lfKoxmzXwP4cmT4T//gRYt/HJeXtCRicQeNfWUCmvmTN9RbP9+3zw0LS26fQREypva+YsUwzl4\n4w0/VET9+v4k0LZt0FGJRIba+YsUwwx694alS+Hyy+Hii/1yTk7QkYkES8lfEkKlSvCnP/nmoWec\nAR06+OX164OOTCQYSv6SUA49FG6+2Z8EjjoKWrf2y5s2BR2ZSHQp+UtCOuIIX/9ftAg2b/Z9BMaM\ngZ07g45MJDqU/CWh1a8Pjz8OH34I8+b55qL/+pf6CEjFp9Y+IgXMneuHkF63zvcW7t1bzUMldqmp\np0gEOQczZviTQFKSbybasycka2ZHiTFK/iLlIC8P3nwTHnjAPxC+4Qa46iqoXj3oyEQ8JX+RcuQc\nfPwxjB0L778P114LQ4f65wUiQVInL5FyZObnD3jjDfjkEz9q6MknQ79+fq5hkXik5C9SCk2bwvjx\nsGYNnHQSnH8+dOoEU6ZoADmJLyr7iByEvXvh3/+Ghx7yfQRGjvRTTR56aNCRSSJQzV8kYM7Be+/5\n5wKffAKDBsHgwX6mMZHyopq/SMDMIDUVJk2CDz6ADRv8fAIDBviZxkRijZK/SISdeCJMmACrVvln\nBGlp0K0bvPuuv0MQiQUq+4iUsz17/IxiDz3kHwqPGgVXXAHVqgUdmcQ71fxF4oBzfnaxsWNhwQL/\nTGDQIDj66KAjk3ilmr9IHDCDLl0gM9MPH/H113DCCTBwoCaXkehT8hcJwEknwRNPwIoVcOyx0LGj\n7zMwa5aeC0h0qOwjEgN27YKXXvIloSpV/HOBPn38zyLFUc1fpILIy/OtgsaOhWXL/BhCAwfCkUcG\nHZnEItX8RSqIpCRIT4fp0yEry5eFUlL8SWDVqqCjk4pEyV8kRrVuDc8+C0uXQq1acNZZcOGFvhOZ\nbpTlYKnsIxIndu6E556DceOgZk3/XKB3b6hcOejIJCiq+YskkLw8P4ro2LF+dNHhw/0cA7VqBR2Z\nRJtq/iIJJCnJTys5ezb85z9+ToEmTfxMY198EXR0Ei/CSv5m1t3McsxspZndXMT7A81skZktMLP3\nzax56PVGZrbTzOaH/kyI9C8gkshOOw1efBEWLYKqVeGMM+CSS2DOnKAjk1hXYtnHzJKAlUBn4Ftg\nLtDHOZdTYJ0azrntoZ97AoOdc+lm1giY7JxrXcJnqOwjEgHbt8Mzz8A//wl16sCNN/qHxJUqBR2Z\nlIfyLvu0BVY5575yzu0DJgK9Cq7wS+IPqQEUnNOoTIGJSOnVqAHDhsHKlfDnP/uTQLNm/u9t24KO\nTmJJOMm/PrCuwPI3odcOYGaDzWw1MAYYXuCtxmY2z8xmm9nZBxWtiIQlORkuvhg+/NCPKDpnDjRu\n7E8IX38ddHQSC8K5GSzqyv1XNRrn3ARggpn1Ae4ArgK+Axo65zab2anAW2bWstCdAgB33XVX/s+p\nqamkpqaGE7+IlODMM+HVV+HLLyEjA045xc8vMGoUnH560NFJaWRnZ5OdnR2RfYVT828H3OWc6x5a\nvgVwzrn7i1nfgM3OuV81PDOz2cCNzrn5hV5XzV8kSn7+GZ58Eh5+GBo18ieBnj393YLEl/Ku+c8F\nUkItd6oAfYBJhQJIKbB4Pv4BMWZWO/TAGDNrAqQAa8sSqIhExuGH+4S/Zo0fNuLvf4fmzeHRR2HH\njqCjk2gpMfk753KBocA0YCkw0Tm33MzuNrPzQ6sNNbMlZjYfuAHoH3r9HGCRmS0AXgMGOue2RPy3\nEJFSq1QJLr3UTzj/7LN+OOnGjeG22+Dbb4OOTsqbeviKSL41a3w56MUX/fwCo0ZBmzZBRyXFUQ9f\nEYmIpk1h/Hh/EmjVyp8AOnf2w0nk5ZW8vcQPXfmLSLH27YPXXvOTz+/cCSNHQr9+cMghQUcmoIHd\nRKScOQfvv+9PAp984ieeHzIE6tYNOrLEprKPiJQrMz/P8KRJvuPYhg3QogVccw0sWRJ0dFIWSv4i\nUionnAATJvghJJo0ga5dfaexadM0yUw8UdlHRA7Knj1+CImHHvIPhUeNgiuugGrVgo6s4lPNX0QC\n5xzMnOknmVmwAAYP9s8Gjj466MgqLtX8RSRwZtClC2Rm+pPA11/7EtHAgZCTU/L2El1K/iIScS1b\nwhNPwIoVcOyxkJrq+wzMmqXnArFCZR8RKXe7d/tew+PG+TuEoUOhb18//4CUnWr+IhIXnPNzDz/y\nCLz3nu8wNmQIpKSUvK38mmr+IhIXzKBTJ3jzTZg/3887fNZZcN55kJWlISSiSVf+IhKoXbt8U9GM\nDD8H8ZAhcNVVULNm0JHFPpV9RCTuOeenm8zIgHffhT59/LOBli2Djix2qewjInHPzJeAXnnFDxlR\np44fUbRLF3jrLcjNDTrCikVX/iISs/buhddf93cD333nO45dcw0cdVTQkcUGXfmLSIVUpYofKmLO\nHH8SWLbMtwy65hpYuDDo6OKbkr+IxIXTT/fTTa5c6U8AF1wAZ58Nr77q5x2Q0lHZR0Ti0v798Pbb\nvs/AypV+GInrroN69YKOLHpU9hGRhFOpEvzhD77T2NSpsH69n2Ogb18/4YyuJ3+brvxFpMLYvBme\neQYefRSOPBKGDYNLL624w0urnb+ISAG5uf5uICPDDy997bVw/fXQoEHQkUWWyj4iIgUkJ/shI6ZO\n9XMPb98OrVtD795+TCFda+rKX0QSxLZt8Pzz/gFx5cq+JHTFFVC9etCRlZ3KPiIiYfplxrGMDPjo\nIz+O0ODBfj7ieKOyj4hImH6Zceztt2HuXEhKgjPPhJ49/ST0iTKyqK78RSTh7dwJL7/s7wb27PEj\ni/bvD4cfHnRkv01lHxGRCHAOPvzQnwRmzIArr/QngubNg46saCr7iIhEgBl06ACvvQaLF0OtWn7+\n4a5dYfLkijWyqK78RUR+w5498O9/+7uBn37yD4cHDPCdyIKmK38RkXJStaofMuLTT/2MY4sWQdOm\n8Kc/+Z/jlZK/iEiY2rb1fQVycqBRI+jRAzp29MNNx9vIoir7iIiU0b59fpaxjAz44gsYNMjfEdSp\nE53PV9lHRCQAlSvDJZf4ISTeeQe+/BJOPBH69fN9CGKZrvxFRCJo0yZ46imYMAHq1vWT0F9yiX92\nEGlq5y8iEmNyc2HKFF8SWrzYTzQzcCDUrx+5z1DZR0QkxiQn+6kmp0/3E85s2gQnnwyXXQYffBD8\nyKK68he3eP+CAAAGZUlEQVQRiZKff4bnnvMjix5yiB9Z9PLL4dBDy7Y/lX1EROJIXp6/I3jkET/l\n5NVX+85jjRuXbj8q+4iIxJGkJOjWzQ8Z8ckn/vnA6adDr15+TKFoXAvryl9EJAbs2AEvveQfEO/f\n71sJ9esHhx1W/DYq+4iIVBDO+akmH3nEPyju29ePLHrCCb9eV2UfEZEKwsyPJPr667BwoZ9m8uyz\noXt333Q0UpPN6MpfRCTG7d4Nr77qS0KbN/s7gQED4IgjVPYREanwnPOji2ZkQGYmbNmi5C8iklC+\n/x6OOaaca/5m1t3McsxspZndXMT7A81skZktMLP3zax5gfduNbNVZrbczLqWJUgRETlQvXoHt32J\nyd/MkoBHgG7AScDlBZN7yEvOudbOuVOAB4BxoW1bApcCLYB0YIKZleksFQuys7ODDiEsijOyFGfk\nxEOMED9xHoxwrvzbAqucc1855/YBE4FeBVdwzm0vsFgD+OV59AXAROfcfufcl8Cq0P7iUrz8h1Cc\nkaU4IyceYoT4ifNgVApjnfrAugLL31BEAjezwcAooDLQqcC2cwqstj70moiIBCicK/+iyjS/ejrr\nnJvgnEsBbgbuKM22IiISXSW29jGzdsBdzrnuoeVbAOecu7+Y9Q3Y7JyrVXhdM5sK3Omc+7TQNjoh\niIiUQVlb+4RT9pkLpJhZI+A7oA9wecEVzCzFObc6tHg+sDL08yTgJTMbhy/3pACfRSp4EREpmxKT\nv3Mu18yGAtPwZaKnnHPLzexuYK5z7h1gqJl1AfYCm4H+oW2XmdlrwDJgHzBYDfpFRIIXE528REQk\nuqI6sFsYncWqmNnEUKewOWbWMJrxlSLO/mb2o5nND/0ZEECMT5nZD2a26DfWGR86lgvNrE004ysQ\nw2/GaWYdzWxLgWP5P9GOMRRHAzObZWbLzGyxmQ0vZr3Ajmk4McbC8TSzqmb2aajT52Izu7OIdQL/\nrocZZ+Df9QKxJIVimFTEe6U/ns65qPzBn2hWA43wzUEXAs0LrXM9MCH082X4PgJRi7EUcfYHxkc7\ntkIxnA20ARYV8346MCX085nAJzEaZ0dgUpDHMhRHPaBN6OcawIoi/t0DPaZhxhgrx/PQ0N/JwCdA\n20LvB/5dDzPOwL/rBWIZCbxY1L9vWY5nNK/8S+wsFlp+LvTz60DnKMb3i3DihKKbsUaNc+5D/POV\n4vQCng+t+ylQ08zqRiO2gsKIEwI+lgDOue+dcwtDP28HlvPrPimBHtMwY4TYOJ47Qz9WxT9bLFxf\njoXvejhxQgwcTzNrAPQAnixmlVIfz2gm/6I6ixX+j5u/jnMuF9hiZkdGJ7xfxxBSVJwAF4du/V8L\n/cPEmsK/Ryx3sGsXuvWeEhoSJFBm1hh/t/Jpobdi5pj+RowQA8czVKJYAHwPTHfOzS20Six818OJ\nE2Ljuz4O+DPF95Mq9fGMZvIPp8NX4XWsiHXKWzhxTgIaO+faADP57xk3lsRLB7t5QCPnx4V6BHgr\nyGDMrAb+ymmEO3DYEoiRY1pCjDFxPJ1zeaEYGgBnFnESioXvejhxBv5dN7PzgB9Cd31G0f8PS308\no5n8vwEKPoRoAHxbaJ11wHEAZpYMHO6cK6lkEGklxumc2xwqCQE8AZwWpdhK4xtCxzKkqOMdOOfc\n9l9uvZ1zWUDlIK4AAcysEj6pvuCce7uIVQI/piXFGEvHMxTDz0A20L3QW7HwXc9XXJwx8l1vD1xg\nZmuBV4Bzzez5QuuU+nhGM/nndxYzsyr4zmKFn1pPJtRHALgEmBXF+H5RYpxmVnAw1V74fgxBKO4q\nAHzM/SC/l/YW59wP0QqskGLjLFgzN7O2+ObHm6IVWCFPA8uccw8X834sHNPfjDEWjqeZ1TazmqGf\nDwG6ADmFVgv8ux5OnLHwXXfO3eaca+ica4LPR7Occ/0KrVbq4xlOD9+IcOF1FnsKeMHMVgEb8b9o\nVIUZ53AzuwDfcW0TcFW04zSzl4FU4Cgz+xq4E6jifwX3uHMu08x6mNlqYAdwdbRjDCdOoLeZXY8/\nlrvwLRWCiLM9cCWwOFQDdsBt+FZfMXFMw4mR2DiexwDPmR8OPgl4NXTsYuq7HmacgX/Xi3Owx1Od\nvEREElBUO3mJiEhsUPIXEUlASv4iIglIyV9EJAEp+YuIJCAlfxGRBKTkLyKSgJT8RUQS0P8DOiKK\nZUowkskAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f46e978ecd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "####################################################\n",
    "\n",
    "print 'Summary Plots'\n",
    "\n",
    "plt.plot(summary_stats.train_loss) # blue\n",
    "plt.plot(summary_stats.valid_loss) # green\n",
    "plt.show()\n",
    "\n",
    "####################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save the predictions to a file for submission\n",
      "2345696/2345796 [============================>.] - ETA: 0sModel Saved\n"
     ]
    }
   ],
   "source": [
    "####################################################\n",
    "\n",
    "print 'Save the predictions to a file for submission'\n",
    "\n",
    "model.load_weights( CNN_best_weights )\n",
    "\n",
    "predictions = model.predict (\n",
    "                             x       = [\n",
    "                                        testing_data_1, \n",
    "                                        testing_data_2, \n",
    "                                        testing_data_1, \n",
    "                                        testing_data_2\n",
    "                                       ], \n",
    "                             verbose = True\n",
    "                            )\n",
    "\n",
    "submission = pd.DataFrame(predictions, columns=['is_duplicate'])\n",
    "submission.insert(0, 'test_id', testing_data_pd.test_id)\n",
    "submission.to_csv('CNN_submission_to_kaggle.csv', index=False)\n",
    "\n",
    "print 'Model Saved'\n",
    "\n",
    "####################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/pynb\n",
      "-rw-rw-r-- 1 ubuntu ubuntu 54242172 Aug 22 14:28 CNN_submission_to_kaggle.csv\n"
     ]
    }
   ],
   "source": [
    "!pwd\n",
    "!ls -l CNN_submission_to_kaggle.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
