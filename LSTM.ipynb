{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import csv\n",
    "import nltk\n",
    "import keras\n",
    "import string\n",
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The function \"text_to_wordlist\" is adapted from:\n",
    "# kaggle.com/currie32/quora-question-pairs/the-importance-of-cleaning-text\n",
    "\n",
    "def text_to_wordlist(\n",
    "                     question, \n",
    "                     remove_stopwords   = False, \n",
    "                     stem_words         = False,\n",
    "                     remove_punctuation = False\n",
    "                    ):\n",
    "\n",
    "    question = re.sub( r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \"          , question)\n",
    "    question = re.sub( r\"what's\"                , \"what is \"   , question)\n",
    "    question = re.sub( r\"\\'s\"                   , \" \"          , question)\n",
    "    question = re.sub( r\"\\'ve\"                  , \" have \"     , question)\n",
    "    question = re.sub( r\"can't\"                 , \"cannot \"    , question)\n",
    "    question = re.sub( r\"n't\"                   , \" not \"      , question)\n",
    "    question = re.sub( r\"i'm\"                   , \"i am \"      , question)\n",
    "    question = re.sub( r\"\\'re\"                  , \" are \"      , question)\n",
    "    question = re.sub( r\"\\'d\"                   , \" would \"    , question)\n",
    "    question = re.sub( r\"\\'ll\"                  , \" will \"     , question)\n",
    "    question = re.sub( r\",\"                     , \" \"          , question)\n",
    "    question = re.sub( r\"\\.\"                    , \" \"          , question)\n",
    "    question = re.sub( r\"!\"                     , \" ! \"        , question)\n",
    "    question = re.sub( r\"\\/\"                    , \" \"          , question)\n",
    "    question = re.sub( r\"\\^\"                    , \" ^ \"        , question)\n",
    "    question = re.sub( r\"\\+\"                    , \" + \"        , question)\n",
    "    question = re.sub( r\"\\-\"                    , \" - \"        , question)\n",
    "    question = re.sub( r\"\\=\"                    , \" = \"        , question)\n",
    "    question = re.sub( r\"'\"                     , \" \"          , question)\n",
    "    question = re.sub( r\"(\\d+)(k)\"              , r\"\\g<1>000\"  , question)\n",
    "    question = re.sub( r\":\"                     , \" : \"        , question)\n",
    "    question = re.sub( r\" e g \"                 , \" eg \"       , question)\n",
    "    question = re.sub( r\" b g \"                 , \" bg \"       , question)\n",
    "    question = re.sub( r\" u s \"                 , \" american \" , question)\n",
    "    question = re.sub( r\"\\0s\"                   , \"0\"          , question)\n",
    "    question = re.sub( r\" 9 11 \"                , \"911\"        , question)\n",
    "    question = re.sub( r\"e - mail\"              , \"email\"      , question)\n",
    "    question = re.sub( r\"j k\"                   , \"jk\"         , question)\n",
    "    question = re.sub( r\"\\s{2,}\"                , \" \"          , question)\n",
    "\n",
    "    if remove_stopwords:\n",
    "        question = question.lower().split()\n",
    "        question = [w for w in question if not w in nltk.corpus.stopwords.words(\"english\")]\n",
    "        question = ' '.join(question)\n",
    "\n",
    "    if stem_words:        \n",
    "        question      = question.lower().split()\n",
    "        stemmed_words = [nltk.stem.SnowballStemmer('english').stemmer.stem(word) for word in question]\n",
    "        question      = ' '.join(stemmed_words)\n",
    "\n",
    "    if remove_punctuation:\n",
    "        question = ''.join([character for character in question if character not in string.punctuation])\n",
    "    \n",
    "    return(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing training dataset\n",
      "Found 404290 question pairs in train.csv\n",
      "Processing test dataset\n",
      "Found 2345796 question pairs in test.csv\n"
     ]
    }
   ],
   "source": [
    "####################################################\n",
    "\n",
    "print 'Processing training dataset'\n",
    "\n",
    "training_questions_1 = [] \n",
    "training_questions_2 = []\n",
    "training_labels      = []\n",
    "\n",
    "with open( '/home/ubuntu/train.csv' ) as f:\n",
    "    \n",
    "    reader = csv.reader(f, delimiter=',')\n",
    "    header = next(reader)\n",
    "    \n",
    "    for row in reader:\n",
    "        \n",
    "        training_questions_1.append( text_to_wordlist ( row[3] ))\n",
    "        training_questions_2.append( text_to_wordlist ( row[4] ))\n",
    "        training_labels.append( int(row[5] ))\n",
    "        \n",
    "print 'Found %s question pairs in train.csv' % len(training_questions_1)\n",
    "\n",
    "####################################################\n",
    "\n",
    "print 'Processing test dataset'\n",
    "\n",
    "test_questions_1  = []\n",
    "test_questions_2  = []\n",
    "test_question_ids = []\n",
    "\n",
    "with open( '/home/ubuntu/test.csv' ) as f:\n",
    "    \n",
    "    reader = csv.reader(f, delimiter=',')\n",
    "    header = next(reader)\n",
    "    \n",
    "    for row in reader:\n",
    "        \n",
    "        test_questions_1.append( text_to_wordlist ( row[1] ))\n",
    "        test_questions_2.append( text_to_wordlist ( row[2] ))\n",
    "        test_question_ids.append( row[0] )\n",
    "        \n",
    "print 'Found %s question pairs in test.csv' % len(test_questions_1)\n",
    "\n",
    "####################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing Words with KERAS\n",
      "Found 120539 unique word tokens\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "maximum_number_of_words = 200000\n",
    "\n",
    "####################################################\n",
    "\n",
    "print 'Tokenizing Words with KERAS'\n",
    "\n",
    "tokenizer = Tokenizer ( num_words = maximum_number_of_words )\n",
    "\n",
    "tokenizer.fit_on_texts ( \n",
    "                        training_questions_1 + \n",
    "                        training_questions_2 + \n",
    "                        test_questions_1     +\n",
    "                        test_questions_2\n",
    "                       )\n",
    "\n",
    "training_sequences_1 = tokenizer.texts_to_sequences( training_questions_1 )\n",
    "training_sequences_2 = tokenizer.texts_to_sequences( training_questions_2 )\n",
    "test_sequences_1     = tokenizer.texts_to_sequences( test_questions_1 )\n",
    "test_sequences_2     = tokenizer.texts_to_sequences( test_questions_2 )\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "print 'Found %s unique word tokens' % len(word_index)\n",
    "\n",
    "####################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Tensors with KERAS\n",
      "Shape of training data tensor: (404290, 30)\n",
      "Shape of testing data tensor: (2345796, 30)\n",
      "Shape of label tensor: (404290,)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "maximum_sequence_length = 30\n",
    "\n",
    "####################################################\n",
    "\n",
    "print 'Creating Tensors with KERAS'\n",
    "\n",
    "training_data_1   = pad_sequences (\n",
    "                                   sequences = training_sequences_1, \n",
    "                                   maxlen    = maximum_sequence_length\n",
    "                                  )\n",
    "\n",
    "training_data_2   = pad_sequences (\n",
    "                                   sequences = training_sequences_2, \n",
    "                                   maxlen    = maximum_sequence_length \n",
    "                                  )\n",
    "\n",
    "training_labels   = np.array( training_labels )\n",
    "\n",
    "test_data_1       = pad_sequences (\n",
    "                                   sequences = test_sequences_1, \n",
    "                                   maxlen    = maximum_sequence_length\n",
    "                                  )\n",
    "\n",
    "test_data_2       = pad_sequences ( \n",
    "                                   sequences = test_sequences_2, \n",
    "                                   maxlen    = maximum_sequence_length\n",
    "                                  )\n",
    "\n",
    "test_question_ids = np.array(test_question_ids)\n",
    "\n",
    "print 'Shape of training data tensor:',  training_data_1.shape\n",
    "print 'Shape of testing data tensor:',   test_data_1.shape\n",
    "print 'Shape of label tensor:',          training_labels.shape\n",
    "\n",
    "####################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors\n",
      "Found 3000000 word vectors of word2vec\n"
     ]
    }
   ],
   "source": [
    "####################################################\n",
    "\n",
    "print 'Indexing word vectors'\n",
    "\n",
    "word2vec = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "                                                           '/home/ubuntu/GoogleNews-vectors-negative300.bin', \n",
    "                                                           binary=True\n",
    "                                                          )\n",
    "\n",
    "print 'Found %s word vectors of word2vec' % len(word2vec.vocab)\n",
    "\n",
    "####################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing embedding matrix\n",
      "Embedding Matrix Shape: (120540, 300)\n"
     ]
    }
   ],
   "source": [
    "embedding_dimension = 300\n",
    "\n",
    "####################################################\n",
    "\n",
    "print 'Preparing embedding matrix'\n",
    "\n",
    "number_of_words  = min( maximum_number_of_words, len(word_index) )+1\n",
    "\n",
    "embedding_matrix = np.zeros( (number_of_words, embedding_dimension) )\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    \n",
    "    if word in word2vec.vocab:\n",
    "        \n",
    "        embedding_matrix[i] = word2vec.word_vec(word)\n",
    "\n",
    "print 'Embedding Matrix Shape:', embedding_matrix.shape\n",
    "\n",
    "####################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared training and validation data matrices\n"
     ]
    }
   ],
   "source": [
    "validation_split = 0.1\n",
    "\n",
    "####################################################\n",
    "\n",
    "print 'Prepared training and validation data matrices'\n",
    "\n",
    "permutations      = np.random.permutation( len(training_data_1) )\n",
    "\n",
    "train_indices     = permutations[:int( len(training_data_1) * (1 - validation_split) )]\n",
    "validate_indicies = permutations[int(  len(training_data_1) * (1 - validation_split) ):]\n",
    "\n",
    "train_data_1      = np.vstack(( training_data_1[ train_indices ], training_data_2[ train_indices ]))\n",
    "train_data_2      = np.vstack(( training_data_2[ train_indices ], training_data_1[ train_indices ]))\n",
    "train_labels      = np.concatenate(( training_labels[ train_indices ], training_labels[ train_indices ]))\n",
    "\n",
    "validate_data_1   = np.vstack(( training_data_1[ validate_indicies ], training_data_2[ validate_indicies ]))\n",
    "validate_data_2   = np.vstack(( training_data_2[ validate_indicies ], training_data_1[ validate_indicies ]))\n",
    "validate_labels   = np.concatenate(( training_labels[ validate_indicies ], training_labels[ validate_indicies ]))\n",
    "\n",
    "weight_values     = np.ones( len(validate_labels) )\n",
    "\n",
    "re_weight         = True # whether to re-weight classes to fit the 17.5% share in test set\n",
    "\n",
    "if re_weight:\n",
    "\n",
    "    weight_values *= 0.472001959\n",
    "    weight_values[validate_labels == 0] = 1.309028344\n",
    "    class_weight = {0: 1.309028344, 1: 0.472001959}\n",
    "    \n",
    "else:\n",
    "    \n",
    "    class_weight = None\n",
    "\n",
    "####################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defined the model\n"
     ]
    }
   ],
   "source": [
    "from keras.layers.merge import concatenate\n",
    "\n",
    "num_lstm        = np.random.randint(175, 275)\n",
    "rate_drop_lstm  = 0.15 + np.random.rand() * 0.25\n",
    "rate_drop_dense = 0.15 + np.random.rand() * 0.25\n",
    "num_dense       = np.random.randint(100, 150)\n",
    "\n",
    "####################################################\n",
    "\n",
    "print 'Defined the model'\n",
    "\n",
    "embedding_layer = keras.layers.Embedding (\n",
    "                                          input_dim             = number_of_words,\n",
    "                                          output_dim            = embedding_dimension,\n",
    "                                          weights               = [embedding_matrix],\n",
    "                                          input_length          = maximum_sequence_length,\n",
    "                                          trainable             = False\n",
    "                                         )\n",
    "\n",
    "lstm_layer      = keras.layers.LSTM      (\n",
    "                                          units                 = num_lstm, \n",
    "                                          dropout               = rate_drop_lstm, \n",
    "                                          recurrent_dropout     = rate_drop_lstm\n",
    "                                         )\n",
    "\n",
    "sequence_1_input     = keras.layers.Input( shape = ( maximum_sequence_length, ), dtype='int32')\n",
    "embedded_sequence_1  = embedding_layer( sequence_1_input )\n",
    "x1                   = lstm_layer( embedded_sequence_1 )\n",
    "\n",
    "sequence_2_input     = keras.layers.Input( shape = ( maximum_sequence_length, ), dtype='int32')\n",
    "embedded_sequence_2  = embedding_layer( sequence_2_input )\n",
    "y1                   = lstm_layer( embedded_sequence_2 )\n",
    "\n",
    "merged_model         = concatenate( [x1, y1] )\n",
    "merged_model         = keras.layers.Dropout( rate_drop_dense )( merged_model )\n",
    "merged_model         = keras.layers.normalization.BatchNormalization()( merged_model )\n",
    "\n",
    "merged_model         = keras.layers.Dense( num_dense, activation='relu' )( merged_model )\n",
    "merged_model         = keras.layers.Dropout( rate_drop_dense )( merged_model )\n",
    "merged_model         = keras.layers.normalization.BatchNormalization()( merged_model )\n",
    "\n",
    "predictions          = keras.layers.Dense( 1, activation='sigmoid' )( merged_model )\n",
    "\n",
    "####################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execute the model\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 30)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_2 (InputLayer)             (None, 30)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)          (None, 30, 300)       36162000    input_1[0][0]                    \n",
      "                                                                   input_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                    (None, 180)           346320      embedding_1[0][0]                \n",
      "                                                                   embedding_1[1][0]                \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)      (None, 360)           0           lstm_1[0][0]                     \n",
      "                                                                   lstm_1[1][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 360)           0           concatenate_1[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNorm (None, 360)           1440        dropout_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 132)           47652       batch_normalization_1[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)              (None, 132)           0           dense_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNorm (None, 132)           528         dropout_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 1)             133         batch_normalization_2[0][0]      \n",
      "====================================================================================================\n",
      "Total params: 36,558,073\n",
      "Trainable params: 395,089\n",
      "Non-trainable params: 36,162,984\n",
      "____________________________________________________________________________________________________\n",
      "Train on 727722 samples, validate on 80858 samples\n",
      "Epoch 1/2\n",
      "727722/727722 [==============================] - 171s - loss: 0.4147 - acc: 0.6836 - val_loss: 0.3848 - val_acc: 0.6692\n",
      "Epoch 2/2\n",
      "727722/727722 [==============================] - 173s - loss: 0.3495 - acc: 0.7145 - val_loss: 0.3269 - val_acc: 0.7468\n",
      "\n",
      " Best Score: 0.326922123592\n"
     ]
    }
   ],
   "source": [
    "####################################################\n",
    "\n",
    "print 'Execute the model'\n",
    "\n",
    "model = keras.models.Model (\n",
    "                            inputs                                    = [sequence_1_input, sequence_2_input], \n",
    "                            outputs                                   = predictions\n",
    "                           )\n",
    "\n",
    "model.compile (\n",
    "               loss                                                   = 'binary_crossentropy',\n",
    "               optimizer                                              = 'nadam',\n",
    "               metrics                                                = ['acc']\n",
    "              )\n",
    "\n",
    "model.summary()\n",
    "\n",
    "early_stopping = keras.callbacks.EarlyStopping (\n",
    "                                                monitor               = 'val_loss', \n",
    "                                                patience              = 3\n",
    "                                               )\n",
    "\n",
    "model_name = 'lstm_%d_%d_%.2f_%.2f' % (\n",
    "                                       num_lstm, \n",
    "                                       num_dense, \n",
    "                                       rate_drop_lstm, \n",
    "                                       rate_drop_dense\n",
    "                                      ) + '.h5'\n",
    "\n",
    "model_checkpoint = keras.callbacks.ModelCheckpoint (\n",
    "                                                    filepath          = model_name, \n",
    "                                                    save_best_only    = True, \n",
    "                                                    save_weights_only = True\n",
    "                                                   )\n",
    "\n",
    "hist = model.fit (\n",
    "                  x                                                   = [train_data_1, train_data_2], \n",
    "                  y                                                   = train_labels,\n",
    "                  epochs                                              = 2, \n",
    "                  batch_size                                          = 2048, \n",
    "                  shuffle                                             = True,\n",
    "                  class_weight                                        = class_weight, \n",
    "                  callbacks                                           = [early_stopping, model_checkpoint],\n",
    "                  validation_data                                     = (\n",
    "                                                                         [validate_data_1, validate_data_2], \n",
    "                                                                         validate_labels, \n",
    "                                                                         weight_values\n",
    "                                                                        )\n",
    "                 )\n",
    "\n",
    "model.load_weights( model_name )\n",
    "\n",
    "print '\\n Best Score:', min(hist.history['val_loss'])\n",
    "\n",
    "####################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save the predictions to a file for submission\n",
      "2345796/2345796 [==============================] - 185s   \n",
      "2345796/2345796 [==============================] - 186s   \n",
      "Model Saved\n"
     ]
    }
   ],
   "source": [
    "####################################################\n",
    "\n",
    "print 'Save the predictions to a file for submission'\n",
    "\n",
    "predictions = model.predict  (\n",
    "                              [test_data_1, test_data_2], \n",
    "                              batch_size = 8192, \n",
    "                              verbose    = 1\n",
    "                             )\n",
    "\n",
    "predictions += model.predict (\n",
    "                              [test_data_2, test_data_1], \n",
    "                              batch_size = 8192, \n",
    "                              verbose    = 1\n",
    "                             )\n",
    "\n",
    "predictions /= 2\n",
    "\n",
    "submission = pd.DataFrame ( {\n",
    "                             'test_id'      : test_question_ids, \n",
    "                             'is_duplicate' : predictions.ravel()\n",
    "                          } )\n",
    "\n",
    "submission.to_csv( 'LSTM_submission_to_kaggle.csv', index=False )\n",
    "\n",
    "print 'Model Saved'\n",
    "\n",
    "####################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/pynb\n",
      "-rw-rw-r-- 1 ubuntu ubuntu 53819462 Aug 11 21:33 LSTM_submission_to_kaggle.csv\n"
     ]
    }
   ],
   "source": [
    "!pwd\n",
    "!ls -l LSTM_submission_to_kaggle.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
