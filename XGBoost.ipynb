{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import nltk\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stops = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "####################################################\n",
    "\n",
    "# determine the number of matching words between question1 and question2 using a simple count and normalize\n",
    "\n",
    "def word_match_simple_count ( row ):\n",
    "    \n",
    "    question1_words = {}\n",
    "    question2_words = {}\n",
    "    \n",
    "    for word in str( row['question1'] ).lower().split():\n",
    "        \n",
    "        if word not in stops:\n",
    "            \n",
    "            question1_words[word] = 1\n",
    "            \n",
    "    for word in str( row['question2'] ).lower().split():\n",
    "        \n",
    "        if word not in stops:\n",
    "            \n",
    "            question2_words[word] = 1\n",
    "            \n",
    "    if len(question1_words) == 0 or len(question2_words) == 0:\n",
    "        return 0\n",
    "\n",
    "    shared_words_in_question1 = [ word for word in question1_words.keys() if word in question2_words ]\n",
    "    shared_words_in_question2 = [ word for word in question2_words.keys() if word in question1_words ]\n",
    "    \n",
    "    return ( len(shared_words_in_question1) + len(shared_words_in_question2) ) / \\\n",
    "           ( len(question1_words)           + len(question2_words)           )\n",
    "\n",
    "####################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####################################################\n",
    "\n",
    "# calculate a weight for each word\n",
    "\n",
    "# If a word frequency is below the minimum count, we ignore the word\n",
    "# smoothing reduces the impact of rare words\n",
    "\n",
    "def get_word_weight ( count, smoothing, minimum_count ):\n",
    "\n",
    "    if count < minimum_count:\n",
    "\n",
    "        return 0\n",
    "    \n",
    "    else:\n",
    "\n",
    "        return 1 / (count + smoothing)\n",
    "\n",
    "####################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stops = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "\n",
    "####################################################\n",
    "\n",
    "# determine the number of matching words between question1 and question2 using a per word weight and normalize\n",
    "\n",
    "def word_match_simple_weight ( row ):\n",
    "    \n",
    "    question1_words = {}\n",
    "    question2_words = {}\n",
    "    \n",
    "    for word in str( row['question1'] ).lower().split():\n",
    "        \n",
    "        if word not in stops:\n",
    "            \n",
    "            question1_words[word] = 1\n",
    "            \n",
    "    for word in str( row['question2'] ).lower().split():\n",
    "        \n",
    "        if word not in stops:\n",
    "            \n",
    "            question2_words[word] = 1\n",
    "            \n",
    "    if len(question1_words) == 0 or len(question2_words) == 0:\n",
    "        return 0\n",
    "    \n",
    "    shared_weights = [ word_weights.get(word, 0) for word in question1_words.keys() if word in question2_words ] + \\\n",
    "                     [ word_weights.get(word, 0) for word in question2_words.keys() if word in question1_words ]\n",
    "        \n",
    "    total_weights  = [ word_weights.get(word, 0) for word in question1_words ] + \\\n",
    "                     [ word_weights.get(word, 0) for word in question2_words ]\n",
    "    \n",
    "    return np.sum( shared_weights ) / np.sum( total_weights )\n",
    "\n",
    "####################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load training and testing dataset\n",
      "Calculate a weight for each word from the training and testing datasets\n"
     ]
    }
   ],
   "source": [
    "####################################################\n",
    "\n",
    "print 'Load training and testing dataset'\n",
    "\n",
    "training_data      = pd.read_csv( '/home/ubuntu/train.csv' )\n",
    "testing_data       = pd.read_csv( '/home/ubuntu/test.csv' )\n",
    "\n",
    "training_questions = pd.Series ( \n",
    "                                training_data['question1'].tolist() +\n",
    "                                training_data['question2'].tolist() \n",
    "                               ).astype(str)\n",
    "\n",
    "testing_questions  = pd.Series ( \n",
    "                                testing_data['question1'].tolist() +\n",
    "                                testing_data['question2'].tolist() \n",
    "                               ).astype(str)\n",
    "\n",
    "####################################################\n",
    "\n",
    "print 'Calculate a weight for each word from the training and testing datasets'\n",
    "\n",
    "word_count         = collections.defaultdict(int)\n",
    "\n",
    "for question in training_questions:\n",
    "    for word in question.lower().split():\n",
    "        word_count[word] += 1\n",
    "\n",
    "for question in testing_questions:\n",
    "    for word in question.lower().split():\n",
    "        word_count[word] += 1\n",
    "\n",
    "word_weights       = {word : get_word_weight ( \n",
    "                                              count, \n",
    "                                              smoothing     = 10000, \n",
    "                                              minimum_count = 2\n",
    "                                             ) for word, count in word_count.items()}\n",
    "\n",
    "####################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepare training and testing data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/IPython/kernel/__main__.py:31: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split the data for training\n",
      "Convert data to XGB format\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "####################################################\n",
    "\n",
    "print 'Prepare training and testing data'\n",
    "\n",
    "x_train                             = pd.DataFrame()\n",
    "x_test                              = pd.DataFrame()\n",
    "\n",
    "####################################################\n",
    "\n",
    "x_train['word_match_simple_count']  = training_data.apply (\n",
    "                                                           func = word_match_simple_count, \n",
    "                                                           axis = 1, \n",
    "                                                           raw  = True\n",
    "                                                          )\n",
    "\n",
    "x_train['word_match_simple_weight'] = training_data.apply (\n",
    "                                                           func = word_match_simple_weight, \n",
    "                                                           axis = 1, \n",
    "                                                           raw  = True\n",
    "                                                          )\n",
    "\n",
    "####################################################\n",
    "\n",
    "x_test['word_match_simple_count']   = testing_data.apply (\n",
    "                                                          func = word_match_simple_count, \n",
    "                                                          axis = 1, \n",
    "                                                          raw  = True\n",
    "                                                         )\n",
    "\n",
    "x_test['word_match_simple_weight']  = testing_data.apply (\n",
    "                                                           func = word_match_simple_weight, \n",
    "                                                           axis = 1, \n",
    "                                                           raw  = True\n",
    "                                                          )\n",
    "\n",
    "####################################################\n",
    "\n",
    "y_train                             = training_data['is_duplicate'].values\n",
    "\n",
    "####################################################\n",
    "\n",
    "print 'Split the data for training'\n",
    "\n",
    "x_train, x_valid, y_train, y_valid  = train_test_split (\n",
    "                                                        x_train,\n",
    "                                                        y_train, \n",
    "                                                        test_size    = 0.2, \n",
    "                                                        random_state = 4242\n",
    "                                                       )\n",
    "####################################################\n",
    "\n",
    "print 'Convert data to XGB format'\n",
    "\n",
    "data_train                          = xgb.DMatrix (\n",
    "                                                   data  = x_train,  \n",
    "                                                   label = y_train\n",
    "                                                  )\n",
    "\n",
    "data_validate                       = xgb.DMatrix (\n",
    "                                                   data  = x_valid, \n",
    "                                                   label = y_valid\n",
    "                                                  )\n",
    "\n",
    "####################################################\n",
    "\n",
    "data_test                           = xgb.DMatrix (\n",
    "                                                   data  = x_test\n",
    "                                                  )\n",
    "\n",
    "####################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execute the XGBoost model\n",
      "[0]\ttrain-logloss:0.686068\tvalid-logloss:0.68614\n",
      "Multiple eval metrics have been passed: 'valid-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-logloss hasn't improved in 50 rounds.\n",
      "[10]\ttrain-logloss:0.628352\tvalid-logloss:0.62904\n",
      "[20]\ttrain-logloss:0.587523\tvalid-logloss:0.588683\n",
      "[30]\ttrain-logloss:0.557667\tvalid-logloss:0.55919\n",
      "[40]\ttrain-logloss:0.535323\tvalid-logloss:0.537145\n",
      "[50]\ttrain-logloss:0.518344\tvalid-logloss:0.520413\n",
      "[60]\ttrain-logloss:0.505285\tvalid-logloss:0.507558\n",
      "[70]\ttrain-logloss:0.495187\tvalid-logloss:0.497627\n",
      "[80]\ttrain-logloss:0.487309\tvalid-logloss:0.489892\n",
      "[90]\ttrain-logloss:0.481158\tvalid-logloss:0.483873\n",
      "[100]\ttrain-logloss:0.476305\tvalid-logloss:0.479135\n",
      "[110]\ttrain-logloss:0.472419\tvalid-logloss:0.475329\n",
      "[120]\ttrain-logloss:0.469358\tvalid-logloss:0.472327\n",
      "[130]\ttrain-logloss:0.466893\tvalid-logloss:0.469915\n",
      "[140]\ttrain-logloss:0.464902\tvalid-logloss:0.467964\n",
      "[150]\ttrain-logloss:0.463318\tvalid-logloss:0.466407\n",
      "[160]\ttrain-logloss:0.462028\tvalid-logloss:0.46515\n",
      "[170]\ttrain-logloss:0.460994\tvalid-logloss:0.464148\n",
      "[180]\ttrain-logloss:0.460139\tvalid-logloss:0.463313\n",
      "[190]\ttrain-logloss:0.459438\tvalid-logloss:0.462637\n",
      "[200]\ttrain-logloss:0.458875\tvalid-logloss:0.462094\n",
      "[210]\ttrain-logloss:0.458389\tvalid-logloss:0.461633\n",
      "[220]\ttrain-logloss:0.457981\tvalid-logloss:0.461249\n",
      "[230]\ttrain-logloss:0.457627\tvalid-logloss:0.460917\n",
      "[240]\ttrain-logloss:0.457341\tvalid-logloss:0.460654\n",
      "[250]\ttrain-logloss:0.457093\tvalid-logloss:0.46043\n",
      "[260]\ttrain-logloss:0.456893\tvalid-logloss:0.46025\n",
      "[270]\ttrain-logloss:0.456716\tvalid-logloss:0.460076\n",
      "[280]\ttrain-logloss:0.456541\tvalid-logloss:0.459914\n",
      "[290]\ttrain-logloss:0.456389\tvalid-logloss:0.45978\n",
      "[300]\ttrain-logloss:0.456263\tvalid-logloss:0.459671\n",
      "[310]\ttrain-logloss:0.456127\tvalid-logloss:0.459553\n",
      "[320]\ttrain-logloss:0.456005\tvalid-logloss:0.459449\n",
      "[330]\ttrain-logloss:0.455885\tvalid-logloss:0.45935\n",
      "[340]\ttrain-logloss:0.45578\tvalid-logloss:0.459265\n",
      "[350]\ttrain-logloss:0.455672\tvalid-logloss:0.459187\n",
      "[360]\ttrain-logloss:0.45558\tvalid-logloss:0.459123\n",
      "[370]\ttrain-logloss:0.455491\tvalid-logloss:0.459065\n",
      "[380]\ttrain-logloss:0.455369\tvalid-logloss:0.458969\n",
      "[390]\ttrain-logloss:0.455249\tvalid-logloss:0.458883\n",
      "[400]\ttrain-logloss:0.455132\tvalid-logloss:0.458788\n",
      "[410]\ttrain-logloss:0.455031\tvalid-logloss:0.45872\n",
      "[420]\ttrain-logloss:0.454932\tvalid-logloss:0.458655\n",
      "[430]\ttrain-logloss:0.454826\tvalid-logloss:0.458577\n",
      "[440]\ttrain-logloss:0.454735\tvalid-logloss:0.458517\n",
      "[450]\ttrain-logloss:0.454645\tvalid-logloss:0.458452\n",
      "[460]\ttrain-logloss:0.454554\tvalid-logloss:0.458389\n",
      "[470]\ttrain-logloss:0.454476\tvalid-logloss:0.458343\n",
      "[480]\ttrain-logloss:0.454377\tvalid-logloss:0.458276\n",
      "[490]\ttrain-logloss:0.454284\tvalid-logloss:0.458211\n",
      "[500]\ttrain-logloss:0.454213\tvalid-logloss:0.458163\n",
      "[510]\ttrain-logloss:0.454164\tvalid-logloss:0.458135\n",
      "[520]\ttrain-logloss:0.454077\tvalid-logloss:0.458069\n",
      "[530]\ttrain-logloss:0.454026\tvalid-logloss:0.458037\n",
      "[540]\ttrain-logloss:0.453945\tvalid-logloss:0.457975\n",
      "[550]\ttrain-logloss:0.453877\tvalid-logloss:0.45793\n",
      "[560]\ttrain-logloss:0.453778\tvalid-logloss:0.457847\n",
      "[570]\ttrain-logloss:0.453711\tvalid-logloss:0.457804\n",
      "[580]\ttrain-logloss:0.453613\tvalid-logloss:0.457725\n",
      "[590]\ttrain-logloss:0.453546\tvalid-logloss:0.457671\n",
      "[600]\ttrain-logloss:0.453469\tvalid-logloss:0.457611\n",
      "[610]\ttrain-logloss:0.453383\tvalid-logloss:0.457542\n",
      "[620]\ttrain-logloss:0.453317\tvalid-logloss:0.457498\n",
      "[630]\ttrain-logloss:0.45326\tvalid-logloss:0.457462\n",
      "[640]\ttrain-logloss:0.453204\tvalid-logloss:0.457428\n",
      "[650]\ttrain-logloss:0.453137\tvalid-logloss:0.457381\n",
      "[660]\ttrain-logloss:0.453087\tvalid-logloss:0.457352\n",
      "[670]\ttrain-logloss:0.453044\tvalid-logloss:0.457334\n",
      "[680]\ttrain-logloss:0.453004\tvalid-logloss:0.457318\n",
      "[690]\ttrain-logloss:0.452961\tvalid-logloss:0.457295\n",
      "[700]\ttrain-logloss:0.452927\tvalid-logloss:0.45728\n",
      "[710]\ttrain-logloss:0.452875\tvalid-logloss:0.457247\n",
      "[720]\ttrain-logloss:0.452837\tvalid-logloss:0.45723\n",
      "[730]\ttrain-logloss:0.452783\tvalid-logloss:0.457194\n",
      "[740]\ttrain-logloss:0.452743\tvalid-logloss:0.457179\n",
      "[750]\ttrain-logloss:0.452704\tvalid-logloss:0.457163\n",
      "[760]\ttrain-logloss:0.452661\tvalid-logloss:0.457137\n",
      "[770]\ttrain-logloss:0.452633\tvalid-logloss:0.457132\n",
      "[780]\ttrain-logloss:0.452582\tvalid-logloss:0.457102\n",
      "[790]\ttrain-logloss:0.45254\tvalid-logloss:0.457089\n",
      "[800]\ttrain-logloss:0.452492\tvalid-logloss:0.457069\n",
      "[810]\ttrain-logloss:0.452452\tvalid-logloss:0.457045\n",
      "[820]\ttrain-logloss:0.452401\tvalid-logloss:0.457011\n",
      "[830]\ttrain-logloss:0.452345\tvalid-logloss:0.456969\n",
      "[840]\ttrain-logloss:0.452299\tvalid-logloss:0.456949\n",
      "[850]\ttrain-logloss:0.452258\tvalid-logloss:0.456931\n",
      "[860]\ttrain-logloss:0.452216\tvalid-logloss:0.456908\n",
      "[870]\ttrain-logloss:0.45218\tvalid-logloss:0.456896\n",
      "[880]\ttrain-logloss:0.452143\tvalid-logloss:0.456878\n",
      "[890]\ttrain-logloss:0.452101\tvalid-logloss:0.456854\n",
      "[900]\ttrain-logloss:0.452063\tvalid-logloss:0.456836\n",
      "[910]\ttrain-logloss:0.452023\tvalid-logloss:0.45682\n",
      "[920]\ttrain-logloss:0.451985\tvalid-logloss:0.456806\n",
      "[930]\ttrain-logloss:0.451951\tvalid-logloss:0.456795\n",
      "[940]\ttrain-logloss:0.451901\tvalid-logloss:0.456766\n",
      "[950]\ttrain-logloss:0.451858\tvalid-logloss:0.45675\n",
      "[960]\ttrain-logloss:0.451804\tvalid-logloss:0.456717\n",
      "[970]\ttrain-logloss:0.451769\tvalid-logloss:0.456705\n",
      "[980]\ttrain-logloss:0.45173\tvalid-logloss:0.456688\n",
      "[990]\ttrain-logloss:0.451694\tvalid-logloss:0.45667\n"
     ]
    }
   ],
   "source": [
    "####################################################\n",
    "\n",
    "print 'Execute the XGBoost model'\n",
    "\n",
    "XGB_parameters                                 = {}\n",
    "XGB_parameters['objective']                    = 'binary:logistic'\n",
    "XGB_parameters['eval_metric']                  = 'logloss'\n",
    "XGB_parameters['eta']                          = 0.02\n",
    "XGB_parameters['max_depth']                    = 4\n",
    "\n",
    "XGB_watchlist                                  = [\n",
    "                                                  (data_train,      'train'), \n",
    "                                                  (data_validate,   'valid')\n",
    "                                                 ]\n",
    "\n",
    "XGB_booster = xgb.train (\n",
    "                         params                = XGB_parameters, \n",
    "                         dtrain                = data_train, \n",
    "                         num_boost_round       = 1000, \n",
    "                         evals                 = XGB_watchlist, \n",
    "                         early_stopping_rounds = 50, \n",
    "                         verbose_eval          = 10\n",
    "                        )\n",
    "\n",
    "####################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make predictions and create submission file\n"
     ]
    }
   ],
   "source": [
    "####################################################\n",
    "\n",
    "print 'Make predictions and create submission file'\n",
    "\n",
    "predictions                    = XGB_booster.predict( data_test )\n",
    "\n",
    "submission                     = pd.DataFrame()\n",
    "\n",
    "submission['test_id']          = testing_data['test_id']\n",
    "submission['is_duplicate']     = predictions\n",
    "\n",
    "submission.to_csv (\n",
    "                   path_or_buf = 'XGBOOST_submission_to_kaggle.csv', \n",
    "                   index       = False\n",
    "                  )\n",
    "\n",
    "####################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
