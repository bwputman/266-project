{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required Packages\n",
    "- spacy: conda install -c conda-forge spacy\n",
    "\n",
    "        # out-of-the-box: download best-matching default model\n",
    "        python -m spacy download en\n",
    "        python -m spacy download de\n",
    "        python -m spacy download fr\n",
    "\n",
    "        # download best-matching version of specific model for your spaCy installation\n",
    "        python -m spacy download en_core_web_md\n",
    "\n",
    "- ipyext: \n",
    "        conda install -c https://conda.anaconda.org/janschulz ipyext\n",
    "\n",
    "- watermark: \n",
    "        pip install watermark\n",
    "\n",
    "- plotly: \n",
    "        conda install -c https://conda.anaconda.org/plotly plotly -n python2.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-18T18:47:37.907509Z",
     "start_time": "2017-08-18T18:47:37.883559Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# install magic extension\n",
    "#!conda install -c https://conda.anaconda.org/janschulz ipyext\n",
    "#!pip install watermark\n",
    "\n",
    "#install plotly\n",
    "#!conda install -c https://conda.anaconda.org/plotly plotly -n python2.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-18T18:47:41.602638Z",
     "start_time": "2017-08-18T18:47:37.911549Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy \n",
    "import re, os, sys\n",
    "\n",
    "import spacy\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from subject_object_extraction import findSVOs\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print the timestamp, server, python version information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-18T18:47:41.647485Z",
     "start_time": "2017-08-18T18:47:41.605697Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last updated: Sat Aug 19 2017 02:47:41 CST\n",
      "\n",
      "CPython 3.5.3\n",
      "IPython 6.1.0\n",
      "\n",
      "nltk 3.2.4\n",
      "scipy 0.19.1\n",
      "pandas 0.20.3\n",
      "spacy 1.9.0\n",
      "numpy 1.13.1\n",
      "\n",
      "compiler   : GCC 4.4.7 20120313 (Red Hat 4.4.7-1)\n",
      "system     : Linux\n",
      "release    : 4.4.0-89-generic\n",
      "machine    : x86_64\n",
      "processor  : x86_64\n",
      "CPU cores  : 6\n",
      "interpreter: 64bit\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "\n",
    "%watermark -u -n -t -z -v -m -p nltk,scipy,pandas,spacy,numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-18T18:47:41.897443Z",
     "start_time": "2017-08-18T18:47:41.650683Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotly imports.\n",
    "# import plotly.offline as plotly\n",
    "from plotly import tools\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.graph_objs as go\n",
    "#from plotly.graph_objs import *\n",
    "\n",
    "init_notebook_mode(connected= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-08-18T18:47:37.929Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://www.dataquest.io/blog/jupyter-notebook-tips-tricks-shortcuts/\n",
    "\n",
    "# enable output for each command lines. By default, IPython only show ouput for the last command in the cell\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all' \n",
    "# InteractiveShell.ast_node_interactivity = 'last' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download stopwords of nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-08-18T18:47:37.938Z"
    }
   },
   "outputs": [],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-08-18T18:47:37.942Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_data = pd.read_csv('./data/train.csv', encoding = 'utf-8').fillna(\"\")\n",
    "testing_data  = pd.read_csv('./data/test.csv', encoding = 'utf-8').fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-08-18T18:47:37.946Z"
    }
   },
   "outputs": [],
   "source": [
    "training_data.head()\n",
    "training_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-08-18T18:47:37.949Z"
    }
   },
   "outputs": [],
   "source": [
    "testing_data.head()\n",
    "testing_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-08-18T18:47:37.953Z"
    }
   },
   "outputs": [],
   "source": [
    "training_data.describe(include='all')\n",
    "testing_data.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-08-18T18:47:37.961Z"
    }
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_md')\n",
    "nltk_stops = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "print('nltk stopwords lenth',len(nltk_stops))\n",
    "\n",
    "# spacy has more stopwords\n",
    "print('spacy stopword lenth',len(spacy.en.word_sets.STOP_WORDS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-08-18T18:47:37.969Z"
    }
   },
   "outputs": [],
   "source": [
    "# To include lower/upper/title -cased words (him/HIM/Him) I had to use:\n",
    "# nlp.vocab.add_flag(lambda s: s.lower() in spacy.en.word_sets.STOP_WORDS, spacy.attrs.IS_STOP)\n",
    "# en_core_web_md does include stopword\n",
    "\n",
    "nlp.vocab.add_flag(lambda s: s.casefold() in spacy.en.word_sets.STOP_WORDS, spacy.attrs.IS_STOP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-08-18T18:47:37.978Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train_q1 = training_data[['id', 'question1']].copy()\n",
    "df_train_q2 = training_data[['id', 'question2']].copy()\n",
    "df_test_q1 = testing_data[['test_id', 'question1']].copy()\n",
    "df_test_q2 = testing_data[['test_id', 'question2']].copy()\n",
    "\n",
    "df_train_q1.columns = ['id', 'question']\n",
    "df_train_q2.columns = ['id', 'question']\n",
    "df_test_q1.columns = ['id', 'question']\n",
    "df_test_q2.columns = ['id', 'question']\n",
    "\n",
    "df_train_q1['dataset'] = 1\n",
    "df_train_q2['dataset'] = 1\n",
    "df_test_q1['dataset'] = 2\n",
    "df_test_q2['dataset'] = 2\n",
    "\n",
    "\n",
    "df_train_q1['q1_or_q2'] = 1\n",
    "df_train_q2['q1_or_q2'] = 2\n",
    "df_test_q1['q1_or_q2'] = 1\n",
    "df_test_q2['q1_or_q2'] = 2\n",
    "\n",
    "\n",
    "df_train_q1.tail()\n",
    "df_train_q2.tail()\n",
    "\n",
    "df_test_q1.tail()\n",
    "df_test_q2.tail()\n",
    "\n",
    "df_all = pd.concat([df_train_q1,  df_train_q2, df_test_q1, df_test_q2])\n",
    "df_all.head()\n",
    "df_all.reset_index(drop=True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-08-18T18:47:37.982Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cleaning data, remove leading and tailing spaces\n",
    "# df_all['q'] = df_all.question.map( lambda q: q.strip().replace(\"\\n\", \" \").replace(\"\\r\", \" \"))\n",
    "\n",
    "df_all['q'] = df_all.question.map( lambda q: re.sub(\"\\s\\s+\" , \" \", q))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-08-18T18:47:37.988Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get the question character length\n",
    "df_all['q_len'] = df_all.q.map(len)\n",
    "df_all.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-08-18T18:47:37.994Z"
    }
   },
   "outputs": [],
   "source": [
    "df_all.pivot_table(values='q', index=['dataset','q1_or_q2'], columns = ['q_len'], \n",
    "               fill_value = 0,\n",
    "               aggfunc='count')\n",
    "\n",
    "df_pivot = df_all.pivot_table(values='id', index=['q'], columns = ['dataset'], \n",
    "               fill_value = 0,\n",
    "#                margins= True, \n",
    "               aggfunc='count')\n",
    "df_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-08-18T18:47:37.998Z"
    }
   },
   "outputs": [],
   "source": [
    "df_dup_pivot = training_data.pivot_table(values='id', index=['is_duplicate'], #columns = ['dataset'],\n",
    "               fill_value = 0,\n",
    "#                margins= True, \n",
    "               aggfunc='count')\n",
    "df_dup_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-08-18T18:47:38.004Z"
    }
   },
   "outputs": [],
   "source": [
    "df_pivot.columns\n",
    "df_pivot.columns = ['1','2']\n",
    "\n",
    "data = go.Bar(x=['Training dataset','Testing dataset'], \n",
    "              y=[sum(df_pivot['1'])/2, sum(df_pivot['2'])/2],\n",
    "#                text = [\"{}\".format(i) for i in question_cnt.index ],\n",
    "              hoverinfo='y+text+name',\n",
    "               name='Counts')\n",
    "layout = go.Layout(\n",
    "    title='Number of Question Pairs',\n",
    "    xaxis=dict(\n",
    "        title='dataset'\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title='Count'\n",
    "    )\n",
    ")\n",
    "iplot(go.Figure(data=[data], layout=layout))\n",
    "\n",
    "\n",
    "###################\n",
    "data = go.Bar(x=['Training dataset','Testing dataset'], \n",
    "              y=[np.array(np.nonzero(df_pivot['1'])).shape[1], \n",
    "                np.array(np.nonzero(df_pivot['2'])).shape[1]], \n",
    "               name='Counts')\n",
    "layout = go.Layout(\n",
    "    title='Number of Unique Questions',\n",
    "    xaxis=dict(\n",
    "        title='dataset'\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title='Numbers of questions'\n",
    "    )\n",
    ")\n",
    "iplot(go.Figure(data=[data], layout=layout))\n",
    "\n",
    "#########################################\n",
    "data = go.Bar(x=['Training dataset','Testing dataset'], \n",
    "              y=[df_pivot.loc['','1'], \n",
    "                df_pivot.loc['','2']], \n",
    "               name='Counts')\n",
    "layout = go.Layout(\n",
    "    title='Number of Empty Questions',\n",
    "    xaxis=dict(\n",
    "        title='dataset'\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title='Numbers of questions'\n",
    "    )\n",
    ")\n",
    "iplot(go.Figure(data=[data], layout=layout))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-08-18T18:47:38.008Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df_pivot = training_data.pivot_table(values='id', index=['is_duplicate'],\n",
    "#                fill_value = 0,\n",
    "#                aggfunc='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-08-18T18:47:38.012Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "top_n = 50\n",
    "\n",
    "# question_val_cnt = df_pivot['1'][df_pivot['1']>0].sort_values(ascending=False)\n",
    "question_val_cnt =  df_all.q[df_all.dataset == 1].value_counts()\n",
    "\n",
    "\n",
    "question_cnt = question_val_cnt[:top_n]\n",
    "\n",
    "data1 = go.Bar(x=[i for i in range(len(question_cnt))], \n",
    "               y=list(question_cnt), \n",
    "               text = [\"{}\".format(i) for i in question_cnt.index ],\n",
    "               name='Counts')\n",
    "\n",
    "appearance_cnt = pd.Series(data=question_val_cnt).value_counts() \n",
    "\n",
    "data2 = go.Bar(x=appearance_cnt.index, \n",
    "               y=appearance_cnt, \n",
    "               name='Counts')\n",
    "\n",
    "fig = tools.make_subplots(rows=2, cols=1,\n",
    "                          subplot_titles=('Most frequent questions', \n",
    "                                                          'Appearance Count'))\n",
    "fig.append_trace(data1, 1, 1)\n",
    "fig.append_trace(data2, 2, 1)\n",
    "\n",
    "\n",
    "fig['layout']['xaxis1'].update(title='questions')\n",
    "fig['layout']['yaxis1'].update(title='Count')\n",
    "\n",
    "fig['layout']['xaxis2'].update(title='Number of occurences of question')\n",
    "fig['layout']['yaxis2'].update(title='Number of questions (log)',\n",
    "                               type='log')\n",
    "\n",
    "\n",
    "fig['layout'].update(title='Training Dataset')\n",
    "\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-08-18T18:47:38.019Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "question_cnt[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the training and testing dataset, many questions appear numerous times. In this section, we will analyze how many times each question appears in the following dataset\n",
    "\n",
    "- training dataset\n",
    "- testing dataset\n",
    "- training + testing dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations - Training dataset\n",
    "\n",
    "In training dataset, the top frequent questions are \n",
    "\n",
    "1. weight loss\n",
    "2. social - Instragram\n",
    "3. weight loss\n",
    "4. money - personal\n",
    "5. social - Instragram\n",
    "6. job\n",
    "7. money - public policy\n",
    "8. education\n",
    "9. health\n",
    "10. social - Instagram\n",
    "\n",
    "If the questions are randomly sampled from Quora, then Weight loss and Instagram(social) seem to the most concerned questions among users.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-08-18T18:47:38.037Z"
    }
   },
   "outputs": [],
   "source": [
    "question_val_cnt =  df_all.q[df_all.dataset == 2].value_counts()\n",
    "\n",
    "question_cnt = question_val_cnt[:top_n]\n",
    "\n",
    "data1 = go.Bar(x=[i for i in range(len(question_cnt))], \n",
    "               y=list(question_cnt), \n",
    "               text = [\"{}\".format(i) for i in question_cnt.index ],\n",
    "               name='Counts')\n",
    "\n",
    "\n",
    "appearance_cnt = pd.Series(data=question_val_cnt).value_counts() \n",
    "\n",
    "data2 = go.Bar(x=appearance_cnt.index, \n",
    "               y=appearance_cnt, \n",
    "               name='Counts')\n",
    "\n",
    "fig = tools.make_subplots(rows=2, cols=1,\n",
    "                          subplot_titles=('Most frequent questions', \n",
    "                                                          'Appearance Count'))\n",
    "fig.append_trace(data1, 1, 1)\n",
    "fig.append_trace(data2, 2, 1)\n",
    "\n",
    "\n",
    "fig['layout']['xaxis1'].update(title='questions')\n",
    "fig['layout']['yaxis1'].update(title='Count')\n",
    "\n",
    "fig['layout']['xaxis2'].update(title='Number of occurences of question')\n",
    "fig['layout']['yaxis2'].update(title='Number of questions (log)',\n",
    "                               type='log')\n",
    "\n",
    "\n",
    "fig['layout'].update(height=1000, width=800,title='Testing Dataset')\n",
    "\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-08-18T18:47:38.041Z"
    }
   },
   "outputs": [],
   "source": [
    "question_cnt[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations - Testing dataset\n",
    "\n",
    "In the testing dataset, top questions are meaningless. Most of them are WH-words questions without noun-phase referring to the subjects/objects. In addition, they are very short, containing one or few words only, and several dont have question mark (?). Only #10 has subject - I.\n",
    "\n",
    "Apprarently, these single WH-word questions are not valid question in Quora. It it likely that these question are added into test dataset to avoid \"cheating\"(i.e. overfitting). These questions are \"noises\" added to the dataset to test the generalization capability of the classification model.\n",
    "\n",
    "From these observations, we could use word count of question and punctuations (e.g. does the question contain question mark ?) as features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-08-18T18:47:38.053Z"
    }
   },
   "outputs": [],
   "source": [
    "question_val_cnt =  df_all.q.value_counts()\n",
    "\n",
    "\n",
    "question_cnt = question_val_cnt[:top_n]\n",
    "\n",
    "data1 = go.Bar(x=[i for i in range(len(question_cnt))], \n",
    "               y=list(question_cnt), \n",
    "               text = [\"{}\".format(i) for i in question_cnt.index ],\n",
    "               name='Counts')\n",
    "\n",
    "\n",
    "appearance_cnt = pd.Series(data=question_val_cnt).value_counts() \n",
    "\n",
    "data2 = go.Bar(x=appearance_cnt.index, \n",
    "               y=appearance_cnt, \n",
    "               name='Counts')\n",
    "\n",
    "fig = tools.make_subplots(rows=2, cols=1,\n",
    "                          subplot_titles=('Most frequent questions', \n",
    "                                                          'Appearance Count'))\n",
    "fig.append_trace(data1, 1, 1)\n",
    "fig.append_trace(data2, 2, 1)\n",
    "\n",
    "\n",
    "fig['layout']['xaxis1'].update(title='questions')\n",
    "fig['layout']['yaxis1'].update(title='Count')\n",
    "\n",
    "fig['layout']['xaxis2'].update(title='Number of occurences of question')\n",
    "fig['layout']['yaxis2'].update(title='Number of questions (log)',\n",
    "                               type='log')\n",
    "\n",
    "\n",
    "fig['layout'].update(height=1000, width=800, title='Training+Testing Dataset')\n",
    "\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-08-18T18:47:38.058Z"
    }
   },
   "outputs": [],
   "source": [
    "question_cnt[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations - Training+Testing dataset\n",
    "\n",
    "WH-words occupies top rankings. In addition, \"What\", \"How\", and ..etc only appear in the testing dataset. The intuition is that we should examine syntactical validility and grammar rules of the questions. We could use Dependency parsing to analyze  the sentence structure and relationship among words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-08-18T18:47:38.072Z"
    }
   },
   "outputs": [],
   "source": [
    "df_all['q_len'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-18T18:39:13.044886Z",
     "start_time": "2017-08-18T18:39:12.959Z"
    }
   },
   "outputs": [],
   "source": [
    "train_q_len = go.Histogram(\n",
    "    x=df_all.q_len[df_all.dataset == 1],\n",
    "    name='train data',\n",
    "    histnorm='probability',\n",
    "    opacity=0.7\n",
    ")\n",
    "test_q_len = go.Histogram(\n",
    "    x=df_all.q_len[df_all.dataset == 2],\n",
    "    name='test data',\n",
    "    histnorm='probability',\n",
    "    opacity=0.7\n",
    ")\n",
    "\n",
    "data = [train_q_len, test_q_len]\n",
    "\n",
    "layout = go.Layout(title='Normalized histogram of character count in questions',\n",
    "                   xaxis=dict(\n",
    "                       title='Number of characters'),\n",
    "                   yaxis=dict(\n",
    "                       title='Probability'))\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "\n",
    "iplot(fig, filename='overlaid histogram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-18T18:39:13.047087Z",
     "start_time": "2017-08-18T18:39:12.963Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nlp_parse(q):\n",
    "    token = []\n",
    "    lemma = []\n",
    "    pos = []\n",
    "    tag =[]\n",
    "    dep = []\n",
    "#     shape = []\n",
    "#     alpha = []\n",
    "    stop =[]\n",
    "    doc = nlp(q)\n",
    "    for w in doc:\n",
    "        token.append(w.text)\n",
    "        lemma.append(w.lemma_)\n",
    "        pos.append(w.pos_)\n",
    "        tag.append(w.tag_)\n",
    "        dep.append(w.dep_)\n",
    "#         shape.append(w.shape_)\n",
    "#         alpha.append(w.is_alpha)\n",
    "        stop.append(w.is_stop)\n",
    "    word_cnt = len(token)\n",
    "    svo = findSVOs(doc)\n",
    "    ents = [ (e.label_, e.text) for e in doc.ents]\n",
    "#     return token, lemma, pos, tag, dep, shape, alpha, stop, word_cnt, svo, ents\n",
    "    return token, lemma, pos, tag, dep, stop, word_cnt, svo, ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-18T18:19:46.372036Z",
     "start_time": "2017-08-18T18:19:46.091567Z"
    }
   },
   "outputs": [],
   "source": [
    "df_ = df_all[(df_all['dataset'] == 1) ].copy()\n",
    "\n",
    "df_.head()\n",
    "len(df_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-18T17:11:20.512793Z",
     "start_time": "2017-08-18T16:53:06.353961Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "df_['token'], df_['lemma'], df_['pos'], \\\n",
    "df_['tag'], df_['dep'], df_['stop'], \\\n",
    "df_['word_cnt'], df_['svo'], df_['ents'] = \\\n",
    "         zip(*df_['q'].map(nlp_parse))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-18T17:45:11.561514Z",
     "start_time": "2017-08-18T17:45:10.043019Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df_[['q','tag','dep','svo','ents']]\n",
    "\n",
    "df_.query('(dataset == 1) & (q_len >0)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-18T17:19:49.306319Z",
     "start_time": "2017-08-18T17:19:36.953868Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Total number of')\n",
    "\n",
    "print('\\t question pairs for training: {}'.format(len( training_data )))\n",
    "print('\\t duplicate question pairs: {:.2%}'.format(training_data['is_duplicate'].mean()))\n",
    "\n",
    "print('####################################################')\n",
    "\n",
    "question_ids = pd.Series( training_data['qid1'].tolist() + training_data['qid2'].tolist() )\n",
    "print('Total number of unique questions in the training data: {}'.format( len(np.unique(question_ids)) ))\n",
    "print('Number of questions that appear multiple times: {}'.format( np.sum(question_ids.value_counts() > 1 )))\n",
    "\n",
    "print('####################################################')\n",
    "\n",
    "training_questions = pd.concat([training_data['question1'], training_data['question2']], \n",
    "                              axis=0, ignore_index = True) \n",
    "\n",
    "testing_questions = pd.concat([testing_data['question1'], testing_data['question2']], \n",
    "                              axis=0, ignore_index = True) \n",
    "\n",
    "print('Training questions with')\n",
    "print('\\t question marks: {:.2%}'.format(np.mean(training_questions.apply(lambda x:1 if '?' in x else 0))))\n",
    "print('\\t [math] tags: {:.2%}'.format(np.mean(training_questions.apply(lambda x: 1 if '[math]' in x else 0 ))))\n",
    "print('\\t full stops: {:.2%}'.format(np.mean(training_questions.apply(lambda x: 1 if '.' in x else 0))))\n",
    "print('\\t numbers: {:.2%}'.format(np.mean(training_questions.apply(lambda x: 1 if len(re.findall('\\d+',x)) else 0))))\n",
    "print('\\t Capital letters: {:.2%}'.format(np.mean(training_questions.apply(lambda x: 1 if len(re.findall('[A-Z]',x)) else 0))))\n",
    "print('\\t capitalised first letters: {:.2%}'.format(np.mean(training_questions.apply(lambda x: 1 if len(re.findall('^[A-Z]',x)) else 0))))\n",
    "\n",
    "empty_q = training_questions.apply(lambda x: 0 if len(x) else 1)\n",
    "print('\\t empty question: {}, {:.4%}'.format(np.sum(empty_q), np.mean(empty_q)))\n",
    "print('####################################################')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-16T01:12:45.039207Z",
     "start_time": "2017-08-16T01:12:45.025771Z"
    }
   },
   "source": [
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Share\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-08-17T22:58:38.803Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_share(q1, q2):\n",
    "    q1_set = set(q1)\n",
    "    q2_set = set(q2)\n",
    "    word_share = q1_set.intersection(q2_set)\n",
    "    return word_share\n",
    "    \n",
    "df_train['word_share'] = df_train.apply(lambda x: word_share(q1 = x['q1_token'], q2 = x['q2_token']), axis=1)\n",
    "df_test['word_share'] = df_test.apply(lambda x: word_share(q1 = x['q1_token'], q2 = x['q2_token']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-16T16:00:39.045967Z",
     "start_time": "2017-08-16T16:00:23.332743Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####################################################\n",
    "\n",
    "training_questions = pd.Series( training_data['question1'].tolist() + training_data['question2'].tolist() ).astype(str)\n",
    "testing_questions  = pd.Series( testing_data['question1'].tolist()  + testing_data['question2'].tolist() ).astype(str)\n",
    "\n",
    "training_distribution = training_questions.apply(lambda x: len(x.split(' ')))\n",
    "testing_distribution  = testing_questions.apply(lambda x: len(x.split(' ')))\n",
    "\n",
    "####################################################\n",
    "\n",
    "plt.hist (\n",
    "          x      = training_distribution, \n",
    "          bins   = 50, \n",
    "          range  = [0, 50], \n",
    "          color  = 'green', \n",
    "          normed = True, \n",
    "          label  = 'training_data'\n",
    "         )\n",
    "\n",
    "plt.hist (\n",
    "          x      = testing_distribution, \n",
    "          bins   = 50, \n",
    "          range  = [0, 50], \n",
    "          color  = 'red', \n",
    "          normed = True, \n",
    "          alpha  = 0.5, \n",
    "          label  = 'testing_data'\n",
    "         )\n",
    "\n",
    "plt.title (\n",
    "           s        = 'Normalised histogram of word count in questions', \n",
    "           fontsize = 15\n",
    "          )\n",
    "\n",
    "plt.xlabel (\n",
    "            s        = 'Number of words', \n",
    "            fontsize = 15\n",
    "           )\n",
    "\n",
    "plt.ylabel (\n",
    "            s        = 'Probability', \n",
    "            fontsize = 15\n",
    "           )\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-14T22:50:58.873581Z",
     "start_time": "2017-08-14T22:50:33.508Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "####################################################\n",
    "\n",
    "def word_match_simple_count ( row ):\n",
    "    \n",
    "    question1_words = {}\n",
    "    question2_words = {}\n",
    "    \n",
    "    for word in str( row['question1'] ).lower().split():\n",
    "        \n",
    "        if word not in stops:\n",
    "            \n",
    "            question1_words[word] = 1\n",
    "            \n",
    "    for word in str( row['question2'] ).lower().split():\n",
    "        \n",
    "        if word not in stops:\n",
    "            \n",
    "            question2_words[word] = 1\n",
    "            \n",
    "    if len(question1_words) == 0 or len(question2_words) == 0:\n",
    "        return 0\n",
    "    \n",
    "    words_shared_question1 = [word for word in question1_words.keys() if word in question2_words]\n",
    "    words_shared_question2 = [word for word in question2_words.keys() if word in question1_words]\n",
    "    \n",
    "    return ( len(words_shared_question1) + len(words_shared_question2) ) / \\\n",
    "           ( len(question1_words)        + len(question2_words)        )\n",
    "\n",
    "####################################################\n",
    "\n",
    "training_data_word_match = training_data.apply (\n",
    "                                                func = word_match_simple_count, \n",
    "                                                axis = 1, \n",
    "                                                raw  = True\n",
    "                                               )\n",
    "\n",
    "plt.hist (\n",
    "          x      = training_data_word_match[training_data['is_duplicate'] == 0], \n",
    "          bins   = 20, \n",
    "          normed = True, \n",
    "          label  = 'Not Duplicate'\n",
    "         )\n",
    "\n",
    "plt.hist ( \n",
    "          x      = training_data_word_match[training_data['is_duplicate'] == 1], \n",
    "          bins   = 20, \n",
    "          normed = True, \n",
    "          alpha  = 0.7, \n",
    "          label  = 'Duplicate'\n",
    "         )\n",
    "\n",
    "plt.title (\n",
    "           s        = 'Label distribution over word_match_share', \n",
    "           fontsize = 15\n",
    "          )\n",
    "\n",
    "plt.xlabel (\n",
    "            s        = 'word_match_share', \n",
    "            fontsize = 15\n",
    "           )\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-14T22:50:58.874885Z",
     "start_time": "2017-08-14T22:50:33.514Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "transformer = TfidfTransformer(smooth_idf=False)\n",
    "\n",
    "transformer \n",
    "\n",
    "#training_questions = pd.Series( training_data['question1'].tolist() + training_data['question2'].tolist() ).astype(str)\n",
    "#testing_questions  = pd.Series( testing_data['question1'].tolist()  + testing_data['question2'].tolist() ).astype(str)\n",
    "\n",
    "counts = [[3, 0, 1],\n",
    "          [2, 0, 0],\n",
    "          [3, 0, 0],\n",
    "          [4, 0, 0],\n",
    "          [3, 2, 0],\n",
    "          [3, 0, 2]]\n",
    "\n",
    "tfidf = transformer.fit_transform(counts)\n",
    "\n",
    "tfidf.toarray() \n",
    "\n",
    "#print tf.get_feature_names()\n",
    "\n",
    "#print len(training_questions)\n",
    "\n",
    "\n",
    "\n",
    "#print tf.get_feature_names()[200:210]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-14T22:50:58.876161Z",
     "start_time": "2017-08-14T22:50:33.517Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Label distribution over word_order_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-14T22:50:58.877691Z",
     "start_time": "2017-08-14T22:50:33.519Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Label distribution over semantic_similarity\n",
    "# http://sujitpal.blogspot.ca/2014/12/semantic-similarity-for-short-sentences.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-14T22:50:58.879789Z",
     "start_time": "2017-08-14T22:50:33.523Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "# tfidf - rare words\n",
    "# from sklearn.feature_extraction.text import TfidfTransformer\n",
    "# https://chisqr.wordpress.com/2017/07/03/classifying-duplicate-questions-with-tensorflow/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-14T22:50:58.881619Z",
     "start_time": "2017-08-14T22:50:33.525Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from difflib import SequenceMatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-14T22:50:58.883298Z",
     "start_time": "2017-08-14T22:50:33.528Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from nltk.corpus import wordnet as wn\n",
    "# nltk.word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-14T22:50:58.884606Z",
     "start_time": "2017-08-14T22:50:33.530Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://github.com/abhishekkrthakur/is_that_a_duplicate_quora_question/blob/master/feature_engineering.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## POS Tag, Lemma, Dependency Parsing Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-15T00:41:15.793463Z",
     "start_time": "2017-08-15T00:41:02.384222Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-15T05:42:06.382133Z",
     "start_time": "2017-08-15T05:42:06.314024Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_data.head()\n",
    "training_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-15T05:42:21.437006Z",
     "start_time": "2017-08-15T05:42:21.399394Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testing_data.head()\n",
    "testing_data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-14T16:47:08.836453Z",
     "start_time": "2017-08-14T16:47:08.829256Z"
    }
   },
   "source": [
    "### 1. Combine training and test data, and remove duplicated questions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-15T00:41:16.272782Z",
     "start_time": "2017-08-15T00:41:15.961957Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_all = pd.concat([training_data.question1, training_data.question2, \n",
    "                    testing_data.question1, testing_data.question2], \n",
    "                   axis =0, ignore_index = True) \n",
    "\n",
    "df_all.reset_index(drop=True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-15T00:41:16.387685Z",
     "start_time": "2017-08-15T00:41:16.279179Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-15T00:41:28.485086Z",
     "start_time": "2017-08-15T00:41:16.392088Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_all.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-15T00:41:43.604074Z",
     "start_time": "2017-08-15T00:41:28.489498Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_no_dup = df_all.drop_duplicates(keep='first') \n",
    "df_no_dup.reset_index(drop=True, inplace = True)\n",
    "df_no_dup.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extract Name Entity information\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-14T22:39:38.604292Z",
     "start_time": "2017-08-14T22:39:38.567664Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df[361520:361530]\n",
    "\n",
    "# for row in tqdm(range(361557,361530)):\n",
    "#     doc = nlp(unicode(df[row], errors='ignore')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Warning, the following code block takes 3 hours to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-15T03:01:26.447031Z",
     "start_time": "2017-08-15T00:49:16.096509Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "ents_dict = defaultdict(lambda : defaultdict(int))\n",
    "\n",
    "df = df_no_dup\n",
    "iter_len = len(df)\n",
    "for row in tqdm(range(0,iter_len)):\n",
    "    try:\n",
    "        if len(df[row]) > 0:\n",
    "            doc = nlp(df[row]) \n",
    "            for ent in doc.ents:\n",
    "                ents_dict[ent.label_][ent.text] += 1\n",
    "    except TypeError:\n",
    "        print(row, df[row])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-15T06:06:55.876473Z",
     "start_time": "2017-08-15T06:06:54.429394Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ents_dict.keys()\n",
    "ents_set = set()\n",
    "for label in ents_dict.keys():\n",
    "    for text in ents_dict[label].keys():  \n",
    "        if not set('[]~!@#$%^&*()_+{}\":;\\'+-<>?').intersection(text):\n",
    "            ents_set.add(text)\n",
    "            \n",
    "# ents_dict\n",
    "len(ents_set) \n",
    "\n",
    "# remove 'US'\n",
    "ents_set.remove('US')\n",
    "ents_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-15T06:10:35.623312Z",
     "start_time": "2017-08-15T06:10:35.615665Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'india' in ents_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-15T09:05:36.906658Z",
     "start_time": "2017-08-15T09:05:36.894842Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_ent(sent): \n",
    "#     print(sent)\n",
    "    sent_new = sent\n",
    "    for ent in ents_set:\n",
    "#         print('\\\\b'+re.escape(ent)+'\\\\b')\n",
    "#        print(ent)\n",
    "        sent_new = re.sub('\\\\b'+ent+'\\\\b', ent, sent_new, flags=re.IGNORECASE|re.MULTILINE|re.X)\n",
    "    return sent_new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-15T09:06:51.215625Z",
     "start_time": "2017-08-15T09:05:38.938113Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = training_data.copy()[:2]\n",
    "\n",
    "# Register `pandas.progress_apply` and `pandas.Series.map_apply` with `tqdm`\n",
    "# (can use `tqdm_gui`, `tqdm_notebook`, optional kwargs, etc.)\n",
    "# tqdm.pandas(desc=\"my bar!\")\n",
    "\n",
    "# Now you can use `progress_apply` instead of `apply`\n",
    "# and `progress_map` instead of `map`\n",
    "# df.progress_apply(lambda x: x**2)\n",
    "\n",
    "df['sent1'] = df.question1.progress_apply(preprocess_ent)\n",
    "# df['sent2'] = df.question2.apply(preprocess_ent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-15T06:07:22.019399Z",
     "start_time": "2017-08-15T06:07:21.983981Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-14T22:45:15.766153Z",
     "start_time": "2017-08-14T22:45:15.687218Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i, row in df.iterrows():\n",
    "    print row[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-15T06:09:41.146747Z",
     "start_time": "2017-08-15T06:09:41.138436Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sent = df.question1[0]\n",
    "sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-15T06:10:02.415229Z",
     "start_time": "2017-08-15T06:10:02.406246Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "re.sub('\\\\b'+'India'+'\\\\b', 'India', sent, flags=re.IGNORECASE|re.MULTILINE|re.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
